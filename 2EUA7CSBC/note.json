{
  "paragraphs": [
    {
      "text": "%md\nIn questo articolo vediamo una sorta di ripasso guidato dell\u0027esame per la certificazione Databricks Spark Associate Developer con Scala. \n\nGli argomenti da conoscere per l\u0027esame di certificazione [CRT020 - Spark certified associated developerwith Scala](https://academy.databricks.com/exam/crt020-scala) sono quelli riportati alla pagina linkata.\n\nIn questo articolo vedremo soltanto le parti di programmazione dell\u0027esame di certificazine",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 14:25:09.380",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eIn questo articolo vediamo una sorta di ripasso guidato dell\u0026rsquo;esame per la certificazione Databricks Spark Associate Developer con Scala. \u003c/p\u003e\n\u003cp\u003eGli argomenti da conoscere per l\u0026rsquo;esame di certificazione \u003ca href\u003d\"https://academy.databricks.com/exam/crt020-scala\"\u003eCRT020 - Spark certified associated developerwith Scala\u003c/a\u003e sono quelli riportati alla pagina linkata.\u003c/p\u003e\n\u003cp\u003eIn questo articolo vedremo soltanto le parti di programmazione dell\u0026rsquo;esame di certificazine\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581806989723_-1592670081",
      "id": "20200215-234949_1538936072",
      "dateCreated": "2020-02-15 23:49:49.723",
      "dateStarted": "2020-02-17 14:25:09.380",
      "dateFinished": "2020-02-17 14:25:09.384",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## SparkContext\nCandidates are expected to know how to use the SparkContext to control basic configuration settings such as spark.sql.shuffle.partitions.",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 14:24:21.780",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eSparkContext\u003c/h2\u003e\n\u003cp\u003eCandidates are expected to know how to use the SparkContext to control basic configuration settings such as spark.sql.shuffle.partitions.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575840608075_-1454541570",
      "id": "20191208-223008_393211527",
      "dateCreated": "2019-12-08 22:30:08.075",
      "dateStarted": "2020-02-17 14:24:21.780",
      "dateFinished": "2020-02-17 14:24:21.783",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nInnanzittuto verifichiamo che versione di Spark stiamo usando:",
      "user": "anonymous",
      "dateUpdated": "2020-02-15 21:15:11.642",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eInnanzittuto verifichiamo che versione di Spark stiamo usando:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581619834115_-396480363",
      "id": "20200213-195034_1977858216",
      "dateCreated": "2020-02-13 19:50:34.115",
      "dateStarted": "2020-02-15 21:15:11.646",
      "dateFinished": "2020-02-15 21:15:18.370",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "spark.version",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 11:21:37.239",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res1: String \u003d 2.4.4\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581619863851_-190976361",
      "id": "20200213-195103_285883926",
      "dateCreated": "2020-02-13 19:51:03.851",
      "dateStarted": "2020-02-17 11:21:37.322",
      "dateFinished": "2020-02-17 11:21:54.842",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nPer settare i parametri di configurazione di Apache Spark posso usare il metodo **spark.conf.set** direttamente nella Spark-shell (o in Zeppelin)",
      "user": "anonymous",
      "dateUpdated": "2020-02-15 21:15:30.041",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003ePer settare i parametri di configurazione di Apache Spark posso usare il metodo \u003cstrong\u003espark.conf.set\u003c/strong\u003e direttamente nella Spark-shell (o in Zeppelin)\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1578909822592_-43330807",
      "id": "20200113-110342_1917604063",
      "dateCreated": "2020-01-13 11:03:42.592",
      "dateStarted": "2020-02-15 21:15:30.041",
      "dateFinished": "2020-02-15 21:15:30.056",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "spark.conf.set(\"spark.sql.shuffle.partitions\", 6)\r\nspark.conf.set(\"spark.executor.memory\", \"6g\")",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 12:05:24.969",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1575843466230_-286619688",
      "id": "20191208-231746_1391846984",
      "dateCreated": "2019-12-08 23:17:46.230",
      "dateStarted": "2020-01-13 12:05:25.391",
      "dateFinished": "2020-01-13 12:05:36.481",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nper verificare che i parametri siano stati settati correttamente uso il metodo **spark.conf.get**",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 11:27:57.142",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eper verificare che i parametri siano stati settati correttamente uso il metodo \u003cstrong\u003espark.conf.get\u003c/strong\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1578910035826_-656022461",
      "id": "20200113-110715_1843470918",
      "dateCreated": "2020-01-13 11:07:15.826",
      "dateStarted": "2020-01-13 11:27:57.143",
      "dateFinished": "2020-01-13 11:27:57.147",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "print(spark.conf.get(\"spark.sql.shuffle.partitions\") + \", \" + spark.conf.get(\"spark.executor.memory\"))",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 11:27:59.102",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "6, 6g"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575843481897_-1352670986",
      "id": "20191208-231801_1338398669",
      "dateCreated": "2019-12-08 23:18:01.897",
      "dateStarted": "2020-01-13 11:28:01.114",
      "dateFinished": "2020-01-13 11:28:09.862",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nPer visualizzare tutti i settaggi posso usare **spark.conf.getAll**",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 11:08:39.804",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003ePer visualizzare tutti i settaggi posso usare \u003cstrong\u003espark.conf.getAll\u003c/strong\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1578910088219_853782121",
      "id": "20200113-110808_1989806295",
      "dateCreated": "2020-01-13 11:08:08.219",
      "dateStarted": "2020-01-13 11:08:39.804",
      "dateFinished": "2020-01-13 11:08:39.812",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "spark.conf.getAll.foreach(println)",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 11:28:13.388",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "(zeppelin.pyspark.python,C:\\Users\\home\\Anaconda3\\envs\\conda2020\\python)\r\n(spark.driver.host,10.0.75.1)\r\n(zeppelin.dep.localrepo,local-repo)\r\n(zeppelin.spark.sql.stacktrace,false)\r\n(spark.driver.port,51622)\r\n(master,local[15])\r\n(spark.repl.class.uri,spark://10.0.75.1:51622/classes)\r\n(zeppelin.spark.useHiveContext,true)\r\n(spark.repl.class.outputDir,C:\\Users\\home\\AppData\\Local\\Temp\\spark6251788337677085436)\r\n(zeppelin.spark.sql.interpolation,false)\r\n(zeppelin.spark.importImplicit,true)\r\n(zeppelin.interpreter.output.limit,102400)\r\n(spark.app.name,Zeppelin)\r\n(zeppelin.R.cmd,R)\r\n(zeppelin.spark.maxResult,1000)\r\n(zeppelin.pyspark.useIPython,true)\r\n(zeppelin.spark.concurrentSQL,false)\r\n(zeppelin.spark.enableSupportedVersionCheck,true)\r\n(zeppelin.spark.printREPLOutput,true)\r\n(zeppelin.dep.additionalRemoteRepository,spark-packages,http://dl.bintray.com/spark-packages/maven,false;)\r\n(org.apache.spark.storage.BlockManager,DEBUG)\r\n(spark.executor.id,driver)\r\n(log4j.logger.org.apache.spark.storage.BlockManager,TRACE)\r\n(zeppelin.spark.useNew,true)\r\n(spark.useHiveContext,true)\r\n(spark.master,local)\r\n(zeppelin.R.image.width,100%)\r\n(zeppelin.spark.ui.hidden,false)\r\n(zeppelin.interpreter.localRepo,C:\\zeppelin-0.8.2-bin-all/local-repo/spark)\r\n(spark.executor.memory,6g)\r\n(spark.driver.allowMultipleContexts,false)\r\n(zeppelin.R.render.options,out.format \u003d \u0027html\u0027, comment \u003d NA, echo \u003d FALSE, results \u003d \u0027asis\u0027, message \u003d F, warning \u003d F, fig.retina \u003d 2)\r\n(zeppelin.interpreter.max.poolsize,10)\r\n(spark.app.id,local-1578911285841)\r\n(zeppelin.R.knitr,true)\r\n(spark.sql.shuffle.partitions,6)\r\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575843504546_-699539699",
      "id": "20191208-231824_1113669698",
      "dateCreated": "2019-12-08 23:18:24.546",
      "dateStarted": "2020-01-13 11:28:13.455",
      "dateFinished": "2020-01-13 11:28:13.974",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nPrima di definire un nuovo SparkContext devo cancellare quello vecchio",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 11:08:59.789",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003ePrima di definire un nuovo SparkContext devo cancellare quello vecchio\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575844875670_1414510794",
      "id": "20191208-234115_293208618",
      "dateCreated": "2019-12-08 23:41:15.670",
      "dateStarted": "2020-01-13 11:08:59.789",
      "dateFinished": "2020-01-13 11:08:59.796",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sc.stop()",
      "user": "anonymous",
      "dateUpdated": "2020-02-13 20:23:58.189",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1575844318442_-2041775792",
      "id": "20191208-233158_1715604013",
      "dateCreated": "2019-12-08 23:31:58.442",
      "dateStarted": "2020-02-13 20:23:58.256",
      "dateFinished": "2020-02-13 20:23:58.457",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nPer definire i parametri di configurazione posso usare `spark.conf` (come fatto sopra) oppure l\u0027oggetto `SparkConf` di **org.apache.spark.SparkConf**, i due sono equivalenti",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 11:12:43.555",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003ePer definire i parametri di configurazione posso usare \u003ccode\u003espark.conf\u003c/code\u003e (come fatto sopra) oppure l\u0026rsquo;oggetto \u003ccode\u003eSparkConf\u003c/code\u003e di \u003cstrong\u003eorg.apache.spark.SparkConf\u003c/strong\u003e, i due sono equivalenti\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575845336548_1893062850",
      "id": "20191208-234856_995728250",
      "dateCreated": "2019-12-08 23:48:56.548",
      "dateStarted": "2020-02-17 11:12:43.556",
      "dateFinished": "2020-02-17 11:12:43.571",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nval conf \u003d new SparkConf().setMaster(\"local[*]\").setAppName(\"provaApp\")\nval sc \u003d new SparkContext(conf)",
      "user": "anonymous",
      "dateUpdated": "2020-02-13 20:24:01.042",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.SparkConf\r\nimport org.apache.spark.SparkContext\r\nconf: org.apache.spark.SparkConf \u003d org.apache.spark.SparkConf@13115401\r\nsc: org.apache.spark.SparkContext \u003d org.apache.spark.SparkContext@5a7f8768\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575843906680_-1369025197",
      "id": "20191208-232506_784248409",
      "dateCreated": "2019-12-08 23:25:06.680",
      "dateStarted": "2020-02-13 20:24:01.111",
      "dateFinished": "2020-02-13 20:24:01.445",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nUn altro modo per visualizzare i settaggi è usare **toDebugString**",
      "user": "anonymous",
      "dateUpdated": "2020-02-13 20:22:40.016",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eUn altro modo per visualizzare i settaggi è usare \u003cstrong\u003etoDebugString\u003c/strong\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1578910676825_331206236",
      "id": "20200113-111756_713292486",
      "dateCreated": "2020-01-13 11:17:56.825",
      "dateStarted": "2020-02-13 20:22:40.016",
      "dateFinished": "2020-02-13 20:22:40.026",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "conf.toDebugString",
      "user": "anonymous",
      "dateUpdated": "2020-02-13 20:24:06.974",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res4: String \u003d\r\nspark.app.name\u003dprovaApp\r\nspark.driver.extraClassPath\u003dC:\\Users\\home\\Downloads\\jar_files\\mongo-spark-connector_2.12-2.4.1.jar\r\nspark.driver.extraJavaOptions\u003d -Dfile.encoding\u003dUTF-8 -Dzeppelin.log.file\u003d\u0027C:\\zeppelin-0.8.2-bin-all\\logs\\zeppelin-interpreter-spark-home-DESKTOP-46GRV54.log\u0027\r\nspark.driver.memory\u003d8g\r\nspark.executor.extraClassPath\u003dC:\\Users\\home\\Downloads\\jar_files\\mongo-spark-connector_2.12-2.4.1.jar\r\nspark.executor.memory\u003d8g\r\nspark.jars\u003dfile:///C:/zeppelin-0.8.2-bin-all/interpreter/spark/spark-interpreter-0.8.2.jar,file:/C:/zeppelin-0.8.2-bin-all/interpreter/spark/spark-interpreter-0.8.2.jar\r\nspark.master\u003dlocal[*]\r\nspark.repl.local.jars\u003dfile:///C:/zeppelin-0.8.2-bin-all/interpreter/spark/spark-interpreter-0.8.2.jar\r\nspark.submit.deployMode\u003dclient\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575845302170_-741980385",
      "id": "20191208-234822_1356753041",
      "dateCreated": "2019-12-08 23:48:22.170",
      "dateStarted": "2020-02-13 20:24:07.045",
      "dateFinished": "2020-02-13 20:24:07.216",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## SparkSession\nSopra abbiamo visto come configurare lo SparkContext, se invece vogliamo definire le proprietà della SparkSession, punto di accesso al cluster di default a partire da Spark 2.0, posso usare il modo che segue. ",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 00:08:17.509",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eSparkSession\u003c/h2\u003e\n\u003cp\u003eSopra abbiamo visto come configurare lo SparkContext, se invece vogliamo definire le proprietà della SparkSession, punto di accesso al cluster di default a partire da Spark 2.0, posso usare il modo che segue.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581619823949_-494476088",
      "id": "20200213-195023_499688790",
      "dateCreated": "2020-02-13 19:50:23.949",
      "dateStarted": "2020-02-14 00:08:17.509",
      "dateFinished": "2020-02-14 00:08:17.514",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "spark.stop()",
      "user": "anonymous",
      "dateUpdated": "2020-02-13 23:57:00.872",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1581633563426_59196553",
      "id": "20200213-233923_1133348979",
      "dateCreated": "2020-02-13 23:39:23.426",
      "dateStarted": "2020-02-13 23:57:00.936",
      "dateFinished": "2020-02-13 23:57:01.155",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\n\nval conf \u003d new SparkConf().setMaster(\"local[*]\").set(\"spark.driver.cores\", \"1\").set(\"spark.executor.memory\",\"4G\")\n\nval spark \u003d SparkSession.builder.config(conf\u003dconf).getOrCreate()",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 00:04:21.632",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.SparkConf\r\nimport org.apache.spark.sql.SparkSession\r\nconf: org.apache.spark.SparkConf \u003d org.apache.spark.SparkConf@650f29a4\r\nspark: org.apache.spark.sql.SparkSession \u003d org.apache.spark.sql.SparkSession@7db780c\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581620054730_1589492117",
      "id": "20200213-195414_690251120",
      "dateCreated": "2020-02-13 19:54:14.730",
      "dateStarted": "2020-02-14 00:04:21.699",
      "dateFinished": "2020-02-14 00:04:21.959",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nNel definire la SparkSession (o lo SparkContext) ci sono due settaggi che sono obbligatori: **spark.master** e **spark.app.name**.\n\nAlcuni parametri dello SparkConf (vedi [link](https://spark.apache.org/docs/latest/configuration.html) per la lista completa):\n\n| property name  | default   | meaning  |\n|---|---|---|\n| spark.app.name  |  none |  |\n| spark.driver.cores  | 1   | Number of cores to use for the driver process, #only in cluster mode.  |\n| spark.driver.memory  | 1g  |   |\n|spark.executor.memory|1g| |\n|spark.local.dir|/tmp|Directory to use for \"scratch\" space in Spark, including map output files and RDDs that get stored on disk. |\n|spark.master|none|The deploy mode of Spark driver program, either \"client\" or \"cluster\", Which means to launch driver program locally (\"client\") or remotely (\"cluster\") on one of the nodes inside the cluster.|\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 00:09:09.045",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eNel definire la SparkSession (o lo SparkContext) ci sono due settaggi che sono obbligatori: \u003cstrong\u003espark.master\u003c/strong\u003e e \u003cstrong\u003espark.app.name\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eAlcuni parametri dello SparkConf (vedi \u003ca href\u003d\"https://spark.apache.org/docs/latest/configuration.html\"\u003elink\u003c/a\u003e per la lista completa):\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth\u003eproperty name \u003c/th\u003e\n      \u003cth\u003edefault \u003c/th\u003e\n      \u003cth\u003emeaning \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003espark.app.name \u003c/td\u003e\n      \u003ctd\u003enone \u003c/td\u003e\n      \u003ctd\u003e \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003espark.driver.cores \u003c/td\u003e\n      \u003ctd\u003e1 \u003c/td\u003e\n      \u003ctd\u003eNumber of cores to use for the driver process, #only in cluster mode. \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003espark.driver.memory \u003c/td\u003e\n      \u003ctd\u003e1g \u003c/td\u003e\n      \u003ctd\u003e \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003espark.executor.memory\u003c/td\u003e\n      \u003ctd\u003e1g\u003c/td\u003e\n      \u003ctd\u003e \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003espark.local.dir\u003c/td\u003e\n      \u003ctd\u003e/tmp\u003c/td\u003e\n      \u003ctd\u003eDirectory to use for \u0026ldquo;scratch\u0026rdquo; space in Spark, including map output files and RDDs that get stored on disk. \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003espark.master\u003c/td\u003e\n      \u003ctd\u003enone\u003c/td\u003e\n      \u003ctd\u003eThe deploy mode of Spark driver program, either \u0026ldquo;client\u0026rdquo; or \u0026ldquo;cluster\u0026rdquo;, Which means to launch driver program locally (\u0026ldquo;client\u0026rdquo;) or remotely (\u0026ldquo;cluster\u0026rdquo;) on one of the nodes inside the cluster.\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575844824061_-734867992",
      "id": "20191208-234024_900986684",
      "dateCreated": "2019-12-08 23:40:24.061",
      "dateStarted": "2020-02-14 00:09:09.046",
      "dateFinished": "2020-02-14 00:09:09.055",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## DataFrame\nCandidates are expected to know how to:\n- Create a DataFrame/Dataset from a collection (e.g. list or set)",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 00:09:25.189",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eDataFrame\u003c/h2\u003e\n\u003cp\u003eCandidates are expected to know how to:\u003cbr/\u003e- Create a DataFrame/Dataset from a collection (e.g. list or set)\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575849278391_-1575998593",
      "id": "20191209-005438_844637410",
      "dateCreated": "2019-12-09 00:54:38.391",
      "dateStarted": "2020-02-14 00:09:25.189",
      "dateFinished": "2020-02-14 00:09:25.194",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nPer la creazione di DataFrame vedere questo [post](!https://medium.com/@mrpowers/manually-creating-spark-dataframes-b14dae906393).\n\nI punti principali sono riportati sotto.",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 11:20:25.716",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003ePer la creazione di DataFrame vedere questo \u003ca href\u003d\"!https://medium.com/@mrpowers/manually-creating-spark-dataframes-b14dae906393\"\u003epost\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI punti principali sono riportati sotto.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575900546889_-1074393814",
      "id": "20191209-150906_329859745",
      "dateCreated": "2019-12-09 15:09:06.889",
      "dateStarted": "2020-01-13 11:20:25.716",
      "dateFinished": "2020-01-13 11:20:25.722",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Creazione di un dataframe - Metodo 1\nPer creare un dataframe da una lista di valori si può usare il metodo **toDF**.\nIn alcuni casi occorre importare spark.implicits._ per poter fare la conversione da lista a datafame.",
      "user": "anonymous",
      "dateUpdated": "2019-12-19 11:02:23.003",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eCreazione di un dataframe - Metodo 1\u003c/h3\u003e\n\u003cp\u003ePer creare un dataframe da una lista di valori si può usare il metodo \u003cstrong\u003etoDF\u003c/strong\u003e.\u003cbr/\u003eIn alcuni casi occorre importare spark.implicits._ per poter fare la conversione da lista a datafame.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575900154389_557036731",
      "id": "20191209-150234_1543111474",
      "dateCreated": "2019-12-09 15:02:34.389",
      "dateStarted": "2019-12-19 11:02:23.006",
      "dateFinished": "2019-12-19 11:02:25.897",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import spark.implicits._\n\nval lista_numeri \u003d List(1,2,3,4,5,6,7)\nval list_df \u003d lista_numeri.toDF()",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 11:28:49.225",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import spark.implicits._\r\nlista_numeri: List[Int] \u003d List(1, 2, 3, 4, 5, 6, 7)\r\nlist_df: org.apache.spark.sql.DataFrame \u003d [value: int]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575899542625_-2071219905",
      "id": "20191209-145222_6272954",
      "dateCreated": "2019-12-09 14:52:22.625",
      "dateStarted": "2020-01-13 11:28:49.289",
      "dateFinished": "2020-01-13 11:28:50.971",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Creazione di un dataframe - Metodo 2\nUso il metodo di [Sparksession](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.SparkSession) **createDataFrame()** definito come:\n`def createDataFrame(rows: rdd[Row], schema: StructType): DataFrame`\nIl metodo necessita due parametri: un **RDD** di **Row** e uno schema definito con **StructType**\nLo schema è definito così:\n`StructType( \n    List ( StructField(\"name1\", TypeName1), StructField(\"name2\", TypeName2) ...) \n)`\n\n",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 11:21:05.965",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eCreazione di un dataframe - Metodo 2\u003c/h3\u003e\n\u003cp\u003eUso il metodo di \u003ca href\u003d\"https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.SparkSession\"\u003eSparksession\u003c/a\u003e \u003cstrong\u003ecreateDataFrame()\u003c/strong\u003e definito come:\u003cbr/\u003e\u003ccode\u003edef createDataFrame(rows: rdd[Row], schema: StructType): DataFrame\u003c/code\u003e\u003cbr/\u003eIl metodo necessita due parametri: un \u003cstrong\u003eRDD\u003c/strong\u003e di \u003cstrong\u003eRow\u003c/strong\u003e e uno schema definito con \u003cstrong\u003eStructType\u003c/strong\u003e\u003cbr/\u003eLo schema è definito così:\u003cbr/\u003e\u003ccode\u003eStructType( \n    List ( StructField(\u0026quot;name1\u0026quot;, TypeName1), StructField(\u0026quot;name2\u0026quot;, TypeName2) ...) \n)\u003c/code\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575901307576_-664743819",
      "id": "20191209-152147_799141739",
      "dateCreated": "2019-12-09 15:21:47.576",
      "dateStarted": "2020-01-13 11:21:05.965",
      "dateFinished": "2020-01-13 11:21:05.972",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.Row\r\nimport org.apache.spark.sql.types.{StructField, StructType, IntegerType, StringType}\r\nimport org.apache.spark.sql.SparkSession\r\n\r\nval spark \u003d SparkSession.builder.master(\"local[15]\").appName(\"test\").getOrCreate()\r\n\r\n//creiamo un RDD da una sequenza\r\nval someData \u003d spark.sparkContext.parallelize( Seq(\r\n                                    Row(10230, \"Paolo\"),\r\n                                    Row(12025, \"Pippo\"),\r\n                                    Row(25876, \"Pedro\")\r\n                                  )\r\n                             )\r\n\r\n//definiamo lo schema\r\nval someSchema \u003d StructType ( List(\r\n                                    StructField(\"Identificativo\", IntegerType, true),\r\n                                    StructField(\"Nome\", StringType, true)\r\n                                    )\r\n                            )\r\n\r\n//dobbiamo dare come parametri un RDD (creato con sc.parallelize) e lo schema\r\nval someDF \u003d spark.createDataFrame( someData, someSchema )\r\n\r\nsomeDF.show()",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 22:16:54.922",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------------+-----+\n|Identificativo| Nome|\n+--------------+-----+\n|         10230|Paolo|\n|         12025|Pippo|\n|         25876|Pedro|\n+--------------+-----+\n\r\nimport org.apache.spark.sql.Row\r\nimport org.apache.spark.sql.types.{StructField, StructType, IntegerType, StringType}\r\nsomeData: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d ParallelCollectionRDD[0] at parallelize at \u003cconsole\u003e:37\r\nsomeSchema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(Identificativo,IntegerType,true), StructField(Nome,StringType,true))\r\nsomeDF: org.apache.spark.sql.DataFrame \u003d [Identificativo: int, Nome: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575900383771_90485770",
      "id": "20191209-150623_1701344785",
      "dateCreated": "2019-12-09 15:06:23.771",
      "dateStarted": "2020-01-13 11:32:16.222",
      "dateFinished": "2020-01-13 11:32:16.935",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nUn altro modo di usare  **StructType** per definire lo schema del dataframe è il seguente:",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 18:52:23.626",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eUn altro modo di usare \u003cstrong\u003eStructType\u003c/strong\u003e per definire lo schema del dataframe è il seguente:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1576169689710_-613879637",
      "id": "20191212-175449_1322998139",
      "dateCreated": "2019-12-12 17:54:49.710",
      "dateStarted": "2020-02-14 18:52:23.628",
      "dateFinished": "2020-02-14 18:52:30.505",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.types.StructType\r\n\r\nval schema \u003d new StructType()\r\n                    .add($\"Identificativo\".long.copy(nullable \u003d false))\r\n                    .add($\"Nome\".string)\r\n                    .add($\"Cognome\".string)\r\n\r\nschema.printTreeString\r\n\r\nimport org.apache.spark.sql.DataFrameReader\r\n\r\nval r: DataFrameReader \u003d spark.read.schema(schema)",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 23:04:03.887",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- Identificativo: long (nullable \u003d false)\n |-- Nome: string (nullable \u003d true)\n |-- Cognome: string (nullable \u003d true)\n\r\nimport org.apache.spark.sql.types.StructType\r\nschema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(Identificativo,LongType,false), StructField(Nome,StringType,true), StructField(Cognome,StringType,true))\r\nimport org.apache.spark.sql.DataFrameReader\r\nr: org.apache.spark.sql.DataFrameReader \u003d org.apache.spark.sql.DataFrameReader@3e061032\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575908601127_309163795",
      "id": "20191209-172321_1676365353",
      "dateCreated": "2019-12-09 17:23:21.127",
      "dateStarted": "2020-02-14 23:04:03.962",
      "dateFinished": "2020-02-14 23:04:04.346",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nAdesso ho un DataFrameReader che posso usare per leggere i miei file",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 00:18:04.592",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eAdesso ho un DataFrameReader che posso usare per leggere i miei file\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581635633087_434721586",
      "id": "20200214-001353_446281350",
      "dateCreated": "2020-02-14 00:13:53.087",
      "dateStarted": "2020-02-14 00:18:04.593",
      "dateFinished": "2020-02-14 00:18:04.599",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val data_from_csv \u003d r.csv(\"exampl1.csv\")\n\n//oppure\n\nval data_from_load \u003d r.load(\"exampl1.csv\")",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 00:18:03.761",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "data_from_csv: org.apache.spark.sql.DataFrame \u003d [Identificativo: bigint, Nome: string ... 1 more field]\r\ndata_from_load: org.apache.spark.sql.DataFrame \u003d [Identificativo: bigint, Nome: string ... 1 more field]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581635755035_-1060938413",
      "id": "20200214-001555_1497513266",
      "dateCreated": "2020-02-14 00:15:55.035",
      "dateStarted": "2020-02-14 00:17:22.683",
      "dateFinished": "2020-02-14 00:17:22.952",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n- creare un dataframe da un Map",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 00:22:19.336",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n  \u003cli\u003ecreare un dataframe da un Map\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581636128519_-1222493439",
      "id": "20200214-002208_1094172672",
      "dateCreated": "2020-02-14 00:22:08.519",
      "dateStarted": "2020-02-14 00:22:19.336",
      "dateFinished": "2020-02-14 00:22:19.350",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nIn questo caso lo schema viene settato automaticamente da Spark.\n\nCon un dato di partenza in formato **Map** lo schema è:\nMap -\u003e Seq -\u003e RDD -\u003e DataFrame",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 22:55:08.338",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eIn questo caso lo schema viene settato automaticamente da Spark.\u003c/p\u003e\n\u003cp\u003eCon un dato di partenza in formato \u003cstrong\u003eMap\u003c/strong\u003e lo schema è:\u003cbr/\u003eMap -\u0026gt; Seq -\u0026gt; RDD -\u0026gt; DataFrame\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575904300804_757742814",
      "id": "20191209-161140_1190648093",
      "dateCreated": "2019-12-09 16:11:40.804",
      "dateStarted": "2020-02-14 22:55:08.338",
      "dateFinished": "2020-02-14 22:55:08.343",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df \u003d spark.createDataFrame( sc.parallelize( Map((\"x\", 24), (\"y\", 25), (\"z\", 26)).toSeq ) )\n\ndf.withColumnRenamed(\"_1\", \"nome\").withColumnRenamed(\"_2\", \"età\").show()",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 22:33:25.194",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+----+---+\n|nome|età|\n+----+---+\n|   x| 24|\n|   y| 25|\n|   z| 26|\n+----+---+\n\r\ndf: org.apache.spark.sql.DataFrame \u003d [_1: string, _2: int]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575902831921_-1934190881",
      "id": "20191209-154711_1751171205",
      "dateCreated": "2019-12-09 15:47:11.921",
      "dateStarted": "2020-02-14 22:33:25.269",
      "dateFinished": "2020-02-14 22:33:27.346",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nin alternativa si possono usare i metodi **toSeq** e **toDF** in cascata (si ricordi di importare **spark.implicits._** prima)",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 22:54:36.976",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003ein alternativa si possono usare i metodi \u003cstrong\u003etoSeq\u003c/strong\u003e e \u003cstrong\u003etoDF\u003c/strong\u003e in cascata (si ricordi di importare \u003cstrong\u003espark.implicits._\u003c/strong\u003e prima)\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581717118753_969365881",
      "id": "20200214-225158_1395418679",
      "dateCreated": "2020-02-14 22:51:58.753",
      "dateStarted": "2020-02-14 22:54:36.976",
      "dateFinished": "2020-02-14 22:54:36.983",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import spark.implicits._\n\nMap((\"x\", 24), (\"y\", 25), (\"z\", 26)).toSeq.toDF",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 22:50:06.123",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import spark.implicits._\r\nres8: org.apache.spark.sql.DataFrame \u003d [_1: string, _2: int]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581716832515_-1582263187",
      "id": "20200214-224712_1822564091",
      "dateCreated": "2020-02-14 22:47:12.515",
      "dateStarted": "2020-02-14 22:50:06.200",
      "dateFinished": "2020-02-14 22:50:07.376",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n- Create a DataFrame for a range of numbers",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 00:21:53.241",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n  \u003cli\u003eCreate a DataFrame for a range of numbers\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575904038036_1544157178",
      "id": "20191209-160718_634751735",
      "dateCreated": "2019-12-09 16:07:18.036",
      "dateStarted": "2020-02-14 00:21:53.241",
      "dateFinished": "2020-02-14 00:21:53.247",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nAnche in questo caso ci sono diversi modi per creare un dataframe con un range di numeri.\n\nNel primo modo si può creare un range, convertirlo in RDD e successivamente usare **.toDF()**",
      "user": "anonymous",
      "dateUpdated": "2020-02-15 21:28:29.901",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eAnche in questo caso ci sono diversi modi per creare un dataframe con un range di numeri.\u003c/p\u003e\n\u003cp\u003eNel primo modo si può creare un range, convertirlo in RDD e successivamente usare \u003cstrong\u003e.toDF()\u003c/strong\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581636106260_1921098456",
      "id": "20200214-002146_1006094405",
      "dateCreated": "2020-02-14 00:21:46.260",
      "dateStarted": "2020-02-15 21:28:29.901",
      "dateFinished": "2020-02-15 21:28:29.909",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sc.parallelize(1 to 10).toDF()",
      "user": "anonymous",
      "dateUpdated": "2020-02-15 21:26:53.381",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res16: org.apache.spark.sql.DataFrame \u003d [value: int]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575906379442_1420602994",
      "id": "20191209-164619_759620927",
      "dateCreated": "2019-12-09 16:46:19.442",
      "dateStarted": "2019-12-09 16:48:07.346",
      "dateFinished": "2019-12-09 16:48:07.548",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\noppure senza passare da un rdd ",
      "user": "anonymous",
      "dateUpdated": "2020-02-15 21:29:37.173",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eoppure senza passare da un rdd\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581798186672_-1080062857",
      "id": "20200215-212306_1759454758",
      "dateCreated": "2020-02-15 21:23:06.672",
      "dateStarted": "2020-02-15 21:29:37.173",
      "dateFinished": "2020-02-15 21:29:37.179",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.DataFrame\n\nval ar \u003d 1 to 10\nval df: DataFrame \u003d ar.toDF()",
      "user": "anonymous",
      "dateUpdated": "2020-02-15 21:23:42.366",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.DataFrame\r\nar: scala.collection.immutable.Range.Inclusive \u003d Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\r\ndf: org.apache.spark.sql.DataFrame \u003d [value: int]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575910062662_1575645885",
      "id": "20191209-174742_65377942",
      "dateCreated": "2019-12-09 17:47:42.662",
      "dateStarted": "2019-12-09 17:50:45.296",
      "dateFinished": "2019-12-09 17:50:45.511",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\ninfine si può usare la funzione **spark.range()**",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 14:22:47.294",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003einfine si può usare la funzione \u003cstrong\u003espark.range()\u003c/strong\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581798593428_916039145",
      "id": "20200215-212953_1282071775",
      "dateCreated": "2020-02-15 21:29:53.429",
      "dateStarted": "2020-02-17 14:22:47.294",
      "dateFinished": "2020-02-17 14:22:47.297",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "spark.range(1, 1000).show(5)",
      "user": "anonymous",
      "dateUpdated": "2020-02-15 21:31:25.435",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---+\n| id|\n+---+\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n+---+\nonly showing top 5 rows\n\r\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581798675327_41311988",
      "id": "20200215-213115_458873053",
      "dateCreated": "2020-02-15 21:31:15.327",
      "dateStarted": "2020-02-15 21:31:25.511",
      "dateFinished": "2020-02-15 21:31:28.801",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\na rigore questo crea un dataset, ma posso usare semplicemente **.toDF** per trasformarlo in dataframe",
      "user": "anonymous",
      "dateUpdated": "2020-02-15 22:43:47.755",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003ea rigore questo crea un dataset, ma posso usare semplicemente \u003cstrong\u003e.toDF\u003c/strong\u003e per trasformarlo in dataframe\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581798653746_-202442460",
      "id": "20200215-213053_406008576",
      "dateCreated": "2020-02-15 21:30:53.747",
      "dateStarted": "2020-02-15 22:43:47.756",
      "dateFinished": "2020-02-15 22:43:47.762",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "spark.range(1,100).toDF(\"ID\").show(5)",
      "user": "anonymous",
      "dateUpdated": "2020-02-15 22:44:55.502",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---+\n| ID|\n+---+\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n+---+\nonly showing top 5 rows\n\r\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1578911779180_228590139",
      "id": "20200113-113619_510501112",
      "dateCreated": "2020-01-13 11:36:19.180",
      "dateStarted": "2020-02-15 22:44:55.576",
      "dateFinished": "2020-02-15 22:44:56.163",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\r\n- Access the DataFrameReaders",
      "user": "anonymous",
      "dateUpdated": "2020-02-15 22:46:44.335",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n  \u003cli\u003eAccess the DataFrameReaders\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581803184746_-283811211",
      "id": "20200215-224624_1199209015",
      "dateCreated": "2020-02-15 22:46:24.746",
      "dateStarted": "2020-02-15 22:46:44.335",
      "dateFinished": "2020-02-15 22:46:44.340",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\r\nIl DataFrameReaders da accesso ad una serie di metodi che consentono di leggere i files contenenti i dati. \r\n\r\nPosso creare il DataFrameReader esplicitamente con:",
      "user": "anonymous",
      "dateUpdated": "2020-02-15 22:46:38.298",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eIl DataFrameReaders da accesso ad una serie di metodi che consentono di leggere i files contenenti i dati. \u003c/p\u003e\n\u003cp\u003ePosso creare il DataFrameReader esplicitamente con:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575908491949_-815642632",
      "id": "20191209-172131_84593321",
      "dateCreated": "2019-12-09 17:21:31.949",
      "dateStarted": "2020-02-15 22:46:38.299",
      "dateFinished": "2020-02-15 22:46:38.305",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.DataFrameReader\n\nval r: DataFrameReader \u003d spark.read\n\nr.csv(\"exampl1.csv\")",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 11:50:38.422",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.DataFrameReader\r\nr: org.apache.spark.sql.DataFrameReader \u003d org.apache.spark.sql.DataFrameReader@3f709ba\r\nres24: org.apache.spark.sql.DataFrame \u003d [_c0: string, _c1: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1578912434517_-443533290",
      "id": "20200113-114714_1780382013",
      "dateCreated": "2020-01-13 11:47:14.517",
      "dateStarted": "2020-01-13 11:50:38.488",
      "dateFinished": "2020-01-13 11:50:39.171",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\noppure implicitamente senza passare da un oggetto DataFrameReader, ma usando **spark.read.csv()**",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 11:51:52.630",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eoppure implicitamente senza passare da un oggetto DataFrameReader, ma usando \u003cstrong\u003espark.read.csv()\u003c/strong\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1578912645998_144834378",
      "id": "20200113-115045_1283655420",
      "dateCreated": "2020-01-13 11:50:45.998",
      "dateStarted": "2020-01-13 11:51:52.631",
      "dateFinished": "2020-01-13 11:51:52.635",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df \u003d spark.read.csv(\"exampl1.csv\")",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 11:51:50.227",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "df: org.apache.spark.sql.DataFrame \u003d [_c0: string, _c1: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575908507408_1287173059",
      "id": "20191209-172147_35366691",
      "dateCreated": "2019-12-09 17:21:47.408",
      "dateStarted": "2020-01-13 11:51:50.290",
      "dateFinished": "2020-01-13 11:51:50.565",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nVediamo un esempio preso da [The Internal of Spark](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataFrameReader.html)",
      "user": "anonymous",
      "dateUpdated": "2020-02-16 23:23:14.597",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eVediamo un esempio preso da \u003ca href\u003d\"https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataFrameReader.html\"\u003eThe Internal of Spark\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575909657365_-515893458",
      "id": "20191209-174057_1442239035",
      "dateCreated": "2019-12-09 17:40:57.366",
      "dateStarted": "2020-02-16 23:23:14.598",
      "dateFinished": "2020-02-16 23:23:14.602",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val csvLine \u003d \"0,Warsaw,Poland\"\r\n\r\nimport org.apache.spark.sql.Dataset\r\nval cities: Dataset[String] \u003d Seq(csvLine).toDS\r\n\r\ncities.show\r\n\r\n// Define schema explicitly (as below)\r\n// or\r\n// option(\"header\", true) + option(\"inferSchema\", true)\r\nimport org.apache.spark.sql.types.StructType\r\nval schema \u003d new StructType()\r\n  .add($\"id\".long.copy(nullable \u003d false))\r\n  .add($\"city\".string)\r\n  .add($\"country\".string)\r\n\r\nschema.printTreeString\r\n\r\nimport org.apache.spark.sql.DataFrame\r\n\r\nval citiesDF: DataFrame \u003d spark\r\n  .read\r\n  .schema(schema)\r\n  .csv(cities)\r\n  \r\ncitiesDF.show",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 12:05:49.004",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---------------+\n|          value|\n+---------------+\n|0,Warsaw,Poland|\n+---------------+\n\r\nroot\n |-- id: long (nullable \u003d false)\n |-- city: string (nullable \u003d true)\n |-- country: string (nullable \u003d true)\n\r\n+---+------+-------+\n| id|  city|country|\n+---+------+-------+\n|  0|Warsaw| Poland|\n+---+------+-------+\n\r\ncsvLine: String \u003d 0,Warsaw,Poland\r\nimport org.apache.spark.sql.Dataset\r\ncities: org.apache.spark.sql.Dataset[String] \u003d [value: string]\r\nimport org.apache.spark.sql.types.StructType\r\nschema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(id,LongType,false), StructField(city,StringType,true), StructField(country,StringType,true))\r\nimport org.apache.spark.sql.DataFrame\r\ncitiesDF: org.apache.spark.sql.DataFrame \u003d [id: bigint, city: string ... 1 more field]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575909412345_-1490276277",
      "id": "20191209-173652_451601145",
      "dateCreated": "2019-12-09 17:36:52.345",
      "dateStarted": "2020-01-13 12:05:49.090",
      "dateFinished": "2020-01-13 12:05:51.525",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n- Register User Defined Functions (UDFs)",
      "user": "anonymous",
      "dateUpdated": "2019-12-09 17:53:15.600",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n  \u003cli\u003eRegister User Defined Functions (UDFs)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575906109867_-1893085455",
      "id": "20191209-164149_1780866584",
      "dateCreated": "2019-12-09 16:41:49.867",
      "dateStarted": "2019-12-09 17:53:15.601",
      "dateFinished": "2019-12-09 17:53:15.611",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nDobbiamo registrare una funzione da noi definita (UDF), che potremo poi usare per operare sui dataframe o in una espressione SQL.\n\nUsiamo il metodo **udf()**, con parametro la funzione da registrare. Nel nostro caso la funzione da registrare è:\n`x(Double) \u003d\u003e {x*x*x}`\n\nIn questo modo registriamo la UDF per l\u0027utilizzo nella dataFrame API. Si badi che non possiamo ancora usare la funzione nelle espressioni SQL.",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 12:08:34.165",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eDobbiamo registrare una funzione da noi definita (UDF), che potremo poi usare per operare sui dataframe o in una espressione SQL.\u003c/p\u003e\n\u003cp\u003eUsiamo il metodo \u003cstrong\u003eudf()\u003c/strong\u003e, con parametro la funzione da registrare. Nel nostro caso la funzione da registrare è:\u003cbr/\u003e\u003ccode\u003ex(Double) \u003d\u0026gt; {x*x*x}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eIn questo modo registriamo la UDF per l\u0026rsquo;utilizzo nella dataFrame API. Si badi che non possiamo ancora usare la funzione nelle espressioni SQL.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575906108831_716586280",
      "id": "20191209-164148_1095261788",
      "dateCreated": "2019-12-09 16:41:48.831",
      "dateStarted": "2020-01-13 12:08:34.165",
      "dateFinished": "2020-01-13 12:08:34.176",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//importiamo udf\nimport org.apache.spark.sql.functions.udf\n\nval power3udf \u003d udf( (x:Double) \u003d\u003e ( x*x*x ) )\n\nval df \u003d spark.sparkContext.parallelize(0 to 10 by 2).toDF()\n\ndf.show()\n\ndf.select(power3udf( $\"value\" )).show()",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 12:06:39.550",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----+\n|value|\n+-----+\n|    0|\n|    2|\n|    4|\n|    6|\n|    8|\n|   10|\n+-----+\n\r\n+----------+\n|UDF(value)|\n+----------+\n|       0.0|\n|       8.0|\n|      64.0|\n|     216.0|\n|     512.0|\n|    1000.0|\n+----------+\n\r\nimport org.apache.spark.sql.functions.udf\r\npower3udf: org.apache.spark.sql.expressions.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,DoubleType,Some(List(DoubleType)))\r\ndf: org.apache.spark.sql.DataFrame \u003d [value: int]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575906104939_842648445",
      "id": "20191209-164144_1827773364",
      "dateCreated": "2019-12-09 16:41:44.939",
      "dateStarted": "2020-01-13 12:06:39.621",
      "dateFinished": "2020-01-13 12:06:40.093",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nDi seguito un altro esempio di registrazione di una UDF, l\u0027esempio è preso dall\u0027articolo di Medium  [Spark User Defined Functions (UDFs)](https://medium.com/@mrpowers/spark-user-defined-functions-udfs-6c849e39443b).\nNell\u0027esempio ho una funzione che converte le stringhe di input in minuscolo e rimuove tutti gli spazi.",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 00:34:30.214",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eDi seguito un altro esempio di registrazione di una UDF, l\u0026rsquo;esempio è preso dall\u0026rsquo;articolo di Medium \u003ca href\u003d\"https://medium.com/@mrpowers/spark-user-defined-functions-udfs-6c849e39443b\"\u003eSpark User Defined Functions (UDFs)\u003c/a\u003e.\u003cbr/\u003eNell\u0026rsquo;esempio ho una funzione che converte le stringhe di input in minuscolo e rimuove tutti gli spazi.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1576172574999_-1894284742",
      "id": "20191212-184254_596233998",
      "dateCreated": "2019-12-12 18:42:54.999",
      "dateStarted": "2020-02-14 00:34:30.215",
      "dateFinished": "2020-02-14 00:34:30.221",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.types.StringType\r\n\r\ndef lowerRemoveAllWhitespace(s: String): String \u003d {\r\n  s.toLowerCase().replaceAll(\"\\\\s\", \"\")\r\n}\r\n\r\nval lowerRemoveAllWhitespaceUDF \u003d udf[String, String](lowerRemoveAllWhitespace)\r\n\r\nval sourceDF \u003d List(\r\n    (\"  HI THERE     \"),\r\n    (\" GivE mE PresenTS     \")).toDF()\r\n\r\nsourceDF.show()\r\n\r\nsourceDF.select(\r\n  lowerRemoveAllWhitespaceUDF(col(\"value\")).as(\"clean_value\")\r\n).show()",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 18:56:41.065",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------------------+\n|               value|\n+--------------------+\n|       HI THERE     |\n| GivE mE PresenTS...|\n+--------------------+\n\r\n+--------------+\n|   clean_value|\n+--------------+\n|       hithere|\n|givemepresents|\n+--------------+\n\r\nimport org.apache.spark.sql.types.StringType\r\nlowerRemoveAllWhitespace: (s: String)String\r\nlowerRemoveAllWhitespaceUDF: org.apache.spark.sql.expressions.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,StringType,Some(List(StringType)))\r\nsourceDF: org.apache.spark.sql.DataFrame \u003d [value: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575917399651_977110280",
      "id": "20191209-194959_1831441693",
      "dateCreated": "2019-12-09 19:49:59.651",
      "dateStarted": "2020-02-14 18:56:41.177",
      "dateFinished": "2020-02-14 18:57:02.569",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nLa funzione è registrata in dataframe. Non ci resta che registrarla anche in SQL.",
      "user": "admin",
      "dateUpdated": "2019-12-12 18:55:15.713",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eLa funzione è registrata in dataframe. Non ci resta che registrarla anche in SQL.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1576173246989_773163977",
      "id": "20191212-185406_539607392",
      "dateCreated": "2019-12-12 18:54:06.989",
      "dateStarted": "2019-12-12 18:55:15.713",
      "dateFinished": "2019-12-12 18:55:15.718",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//creiamo un dataframe\r\nval df \u003d (1 to 100 by 3).toDF(\"num\")\r\ndf.show(5)\r\n\r\n//registrimo la funzione in SQL\r\nspark.udf.register(\"power2\", (x: Double) \u003d\u003e x*x)\r\n\r\n//adesso che la funzione è registrata posso usarla in una query sql, nel formato Scala\r\ndf.selectExpr(\"power2(num)\").show(5)\r\n\r\n//posso usare la funzione anche nel formato SQL\r\n//prima creiamo una view temporanea\r\ndf.createOrReplaceTempView(\"udfExampleSQL3\")\r\n//e poi la query\r\nspark.sql(\"SELECT power2(num) AS p2 FROM udfExampleSQL3\").show(5)",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 12:11:43.320",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---+\n|num|\n+---+\n|  1|\n|  4|\n|  7|\n| 10|\n| 13|\n+---+\nonly showing top 5 rows\n\r\n+-------------------------------+\n|UDF:power2(cast(num as double))|\n+-------------------------------+\n|                            1.0|\n|                           16.0|\n|                           49.0|\n|                          100.0|\n|                          169.0|\n+-------------------------------+\nonly showing top 5 rows\n\r\n+-----+\n|   p2|\n+-----+\n|  1.0|\n| 16.0|\n| 49.0|\n|100.0|\n|169.0|\n+-----+\nonly showing top 5 rows\n\r\ndf: org.apache.spark.sql.DataFrame \u003d [num: int]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575912535969_-1502572621",
      "id": "20191209-182855_435960561",
      "dateCreated": "2019-12-09 18:28:55.969",
      "dateStarted": "2020-01-13 12:11:43.390",
      "dateFinished": "2020-01-13 12:11:43.748",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\r\n## DataFrameReader\r\n\r\n### Read data for the \"core\" data formats (CSV, JSON, JDBC, ORC, Parquet, text and tables)",
      "user": "admin",
      "dateUpdated": "2019-12-12 19:11:47.142",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eDataFrameReader\u003c/h2\u003e\n\u003ch3\u003eRead data for the \u0026ldquo;core\u0026rdquo; data formats (CSV, JSON, JDBC, ORC, Parquet, text and tables)\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575926947456_-1243108255",
      "id": "20191209-222907_932594899",
      "dateCreated": "2019-12-09 22:29:07.456",
      "dateStarted": "2019-12-12 19:11:47.142",
      "dateFinished": "2019-12-12 19:11:47.146",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nIn modo generico questo è il comando per leggere dati da una sorgente esterna in un dataframe:\n\n```\nspark.read.format(\"csv\")\n    .option(\"mode\", \"FAILFAST\")\n    .option(\"inferSchema\", \"true\")\n    .option(\"path\", \"path/to/file(s)\")\n    .schema(someSchema)\n    .load()\n```\n\nUna delle opzioni è il read `mode`, che specifica cosa fare in caso di dati malformati. Queste sono le possibilità:\n\n- *permissive* (DEFAULT), Sets all fields to null when it encounters a corrupted record and places all corrupted records in a string column called _corrupt_record\n- *dropMalformed*, non importa i dati malformati\n- *failfast*, da un messaggio di errore ed interrompe la procedura di lettura\n\nQuesti sono i comandi per leggere i formati più comuni:",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 14:16:58.509",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eIn modo generico questo è il comando per leggere dati da una sorgente esterna in un dataframe:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003espark.read.format(\u0026quot;csv\u0026quot;)\n    .option(\u0026quot;mode\u0026quot;, \u0026quot;FAILFAST\u0026quot;)\n    .option(\u0026quot;inferSchema\u0026quot;, \u0026quot;true\u0026quot;)\n    .option(\u0026quot;path\u0026quot;, \u0026quot;path/to/file(s)\u0026quot;)\n    .schema(someSchema)\n    .load()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUna delle opzioni è il read \u003ccode\u003emode\u003c/code\u003e, che specifica cosa fare in caso di dati malformati. Queste sono le possibilità:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003cem\u003epermissive\u003c/em\u003e (DEFAULT), Sets all fields to null when it encounters a corrupted record and places all corrupted records in a string column called _corrupt_record\n\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003edropMalformed\u003c/em\u003e, non importa i dati malformati\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003efailfast\u003c/em\u003e, da un messaggio di errore ed interrompe la procedura di lettura\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eQuesti sono i comandi per leggere i formati più comuni:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581887484727_542797385",
      "id": "20200216-221124_130212794",
      "dateCreated": "2020-02-16 22:11:24.727",
      "dateStarted": "2020-02-17 14:16:58.509",
      "dateFinished": "2020-02-17 14:16:58.518",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val dataFrame \u003d spark.read.json(\"example.json\")\r\n\r\nval dataFrame \u003d spark.read.csv(\"example.csv\") \r\n\r\nval dataFrame \u003d spark.read.parquet(\"example.parquet\")\r\n\r\nval dataFrame \u003d spark.read.text(\"file.txt\")\r\n\r\nval dataFrame \u003d spark.read.orc(\"file.orc\")\r\n\r\nval dataFrame \u003d spark.read.jdbc(url,\"person\",properties)",
      "user": "admin",
      "dateUpdated": "2020-02-14 22:15:13.107",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1575929693834_-1523636789",
      "id": "20191209-231453_154806882",
      "dateCreated": "2019-12-09 23:14:53.834",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nPer i dettagli riguardo il connettere Spark ad un database SQL usando **jdbc** si può fare riferimento a questo [link](https://docs.databricks.com/data/data-sources/sql-databases.html)\n\nUna sintassi equivalente per importare i file è la seguente:",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 14:14:52.644",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003ePer i dettagli riguardo il connettere Spark ad un database SQL usando \u003cstrong\u003ejdbc\u003c/strong\u003e si può fare riferimento a questo \u003ca href\u003d\"https://docs.databricks.com/data/data-sources/sql-databases.html\"\u003elink\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eUna sintassi equivalente per importare i file è la seguente:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581714678051_105082772",
      "id": "20200214-221118_1551087106",
      "dateCreated": "2020-02-14 22:11:18.051",
      "dateStarted": "2020-02-17 14:14:52.644",
      "dateFinished": "2020-02-17 14:14:52.648",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "spark.read.format(\"json\").load(\"example.json\")\n\nspark.read.format(\"csv\").load(\"example.csv\")\n\nspark.read.format(\"parquet\").load(\"example.parquet\")\n\nspark.read.format(\"text\").load(\"file.txt\")",
      "user": "anonymous",
      "dateUpdated": "2020-02-14 22:15:21.824",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1581714913272_85899613",
      "id": "20200214-221513_1569252598",
      "dateCreated": "2020-02-14 22:15:13.272",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\r\n### How to configure options for specific formats\r\n",
      "user": "admin",
      "dateUpdated": "2019-12-12 19:11:57.257",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "databaseName": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eHow to configure options for specific formats\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575927262915_1309406287",
      "id": "20191209-223422_742724847",
      "dateCreated": "2019-12-09 22:34:22.915",
      "dateStarted": "2019-12-12 19:11:57.258",
      "dateFinished": "2019-12-12 19:11:57.262",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nDevo usare il metodo **option(\"parametro\", \"valore\")**, mettendo anche più option in cascata.",
      "user": "admin",
      "dateUpdated": "2020-02-17 14:17:07.147",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eDevo usare il metodo \u003cstrong\u003eoption(\u0026ldquo;parametro\u0026rdquo;, \u0026ldquo;valore\u0026rdquo;)\u003c/strong\u003e, mettendo anche più option in cascata.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1576174228562_-1715279243",
      "id": "20191212-191028_108858606",
      "dateCreated": "2019-12-12 19:10:28.562",
      "dateStarted": "2019-12-12 19:11:16.963",
      "dateFinished": "2019-12-12 19:11:16.967",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df \u003d spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"exampl1.csv\")\n//or \nval df2 \u003d spark.read.options(Map((\"header\", \"true\"), (\"inferSchema\",\"true\"))).csv(\"exampl1.csv\")\n//or\nval df3 \u003d spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"mode\", \"FAILFAST\").load(\"exampl1.csv\")",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 10:12:20.213",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "df: org.apache.spark.sql.DataFrame \u003d [eta: int, amici: int]\r\ndf2: org.apache.spark.sql.DataFrame \u003d [eta: int, amici: int]\r\ndf3: org.apache.spark.sql.DataFrame \u003d [eta: int, amici: int]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575929691444_-1388387565",
      "id": "20191209-231451_1479988188",
      "dateCreated": "2019-12-09 23:14:51.444",
      "dateStarted": "2019-12-10 10:12:20.257",
      "dateFinished": "2019-12-10 10:12:20.728",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nPer vedere le opzioni disponibili per i metodi di [DataFrameReader](https://spark.apache.org/docs/2.2.1/api/scala/#org.apache.spark.sql.DataFrameReader), ad esempio csv, si può fare riferimento alla documentazione della API.\nAd esempio per conoscere tutte le opzioni disponibili per csv si può clikkare sulla freccia per espandere il campo csv (vedere la figura sotto).\n![figura](https://www.1week4.com/wp-content/uploads/2019/12/spark-api-dataframereader-options.jpg)",
      "user": "admin",
      "dateUpdated": "2019-12-12 19:09:38.202",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003ePer vedere le opzioni disponibili per i metodi di \u003ca href\u003d\"https://spark.apache.org/docs/2.2.1/api/scala/#org.apache.spark.sql.DataFrameReader\"\u003eDataFrameReader\u003c/a\u003e, ad esempio csv, si può fare riferimento alla documentazione della API.\u003cbr/\u003eAd esempio per conoscere tutte le opzioni disponibili per csv si può clikkare sulla freccia per espandere il campo csv (vedere la figura sotto).\u003cbr/\u003e\u003cimg src\u003d\"https://www.1week4.com/wp-content/uploads/2019/12/spark-api-dataframereader-options.jpg\" alt\u003d\"figura\" /\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575931195975_-81848269",
      "id": "20191209-233955_1825649355",
      "dateCreated": "2019-12-09 23:39:55.975",
      "dateStarted": "2019-12-12 19:09:38.202",
      "dateFinished": "2019-12-12 19:09:38.210",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nAlcune delle opzioni sono per l\u0027importazione dei file csv sono:\n- header\n- inferSchema\n- mode\n- ...",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 11:12:42.307",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eAlcune delle opzioni sono per l\u0026rsquo;importazione dei file csv sono:\u003cbr/\u003e- header\u003cbr/\u003e- inferSchema\u003cbr/\u003e- mode\u003cbr/\u003e- \u0026hellip;\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575926647615_-843164516",
      "id": "20191209-222407_39639960",
      "dateCreated": "2019-12-09 22:24:07.615",
      "dateStarted": "2019-12-10 11:12:42.308",
      "dateFinished": "2019-12-10 11:12:42.314",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 14:17:20.958",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1581945336578_76048105",
      "id": "20200217-141536_1783206713",
      "dateCreated": "2020-02-17 14:15:36.578",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### How to read data from non-core formats using format() and load()",
      "user": "admin",
      "dateUpdated": "2019-12-12 19:12:08.478",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eHow to read data from non-core formats using format() and load()\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575928614828_-796565143",
      "id": "20191209-225654_1498953618",
      "dateCreated": "2019-12-09 22:56:54.828",
      "dateStarted": "2019-12-12 19:12:08.478",
      "dateFinished": "2019-12-12 19:12:08.482",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nSupponiamo di avere un file in cui i dati sono salvati seguendo un formato particolare, per esempio:\n- il separatore è il segno meno **-** invece della virgola **,**\n- nel file ci sono linee di commenti che iniziano con **\u003c!--**\n- il file è indentato per cui ci sono spazi vuoti all\u0027inizio delle righe\n- etc.\n\nper ognuno di questi casi particolari trovo una opzione di **load**.",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 11:28:05.152",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eSupponiamo di avere un file in cui i dati sono salvati seguendo un formato particolare, per esempio:\u003cbr/\u003e- il separatore è il segno meno \u003cstrong\u003e-\u003c/strong\u003e invece della virgola \u003cstrong\u003e,\u003c/strong\u003e\u003cbr/\u003e- nel file ci sono linee di commenti che iniziano con \u003cstrong\u003e\u0026lt;!\u0026ndash;\u003c/strong\u003e\u003cbr/\u003e- il file è indentato per cui ci sono spazi vuoti all\u0026rsquo;inizio delle righe\u003cbr/\u003e- etc.\u003c/p\u003e\n\u003cp\u003eper ognuno di questi casi particolari trovo una opzione di \u003cstrong\u003eload\u003c/strong\u003e.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575973368689_47263036",
      "id": "20191210-112248_463581520",
      "dateCreated": "2019-12-10 11:22:48.689",
      "dateStarted": "2019-12-10 11:28:05.152",
      "dateFinished": "2019-12-10 11:28:05.163",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df \u003d spark.read.format(\"txt\").option(\"sep\",\"-\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").option(\"ignoreLeadingWhiteSpace\",\"true\").option(\"comment\", \"\u003c!--\").load(data_file)",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 11:27:01.385",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1575972833639_133509361",
      "id": "20191210-111353_98572472",
      "dateCreated": "2019-12-10 11:13:53.639",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\r\n### How to construct and specify a schema using the StructType classes\r\n",
      "user": "admin",
      "dateUpdated": "2019-12-12 19:12:18.688",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eHow to construct and specify a schema using the StructType classes\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575972832766_210402550",
      "id": "20191210-111352_766045483",
      "dateCreated": "2019-12-10 11:13:52.766",
      "dateStarted": "2019-12-12 19:12:18.689",
      "dateFinished": "2019-12-12 19:12:18.692",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, FloatType}\n\nval my_schema \u003d StructType( List(\n                                StructField(\"nome\", StringType),\n                                StructField(\"cognome\", StringType, nullable\u003dtrue),\n                                StructField(\"altezza\", IntegerType, nullable\u003dtrue), \n                                StructField(\"peso\", FloatType, nullable\u003dtrue),\n                                StructField(\"età\", IntegerType)\n                                )\n                          )\n\nval df \u003d spark.read.format(\"csv\").schema(my_schema).option(\"mode\",\"PERMISSIVE\").load(\"exampl2.csv\")\ndf.show()",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 11:50:05.251",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------+---------+-------+----+---+\n|    nome|  cognome|altezza|peso|età|\n+--------+---------+-------+----+---+\n|   Pippo|  Paolini|    167|72.3| 44|\n|   Luigi|    Russo|    178|89.2|  5|\n|Giovanna|    Rosso|    175|80.0| 44|\n|Giuseppe|  Bianchi|    165|75.8|  9|\n|  Amedeo|    Verdi|    167|56.2| 18|\n|   Luisa|Valentino|    182|64.9| 15|\n+--------+---------+-------+----+---+\n\r\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, FloatType}\r\nmy_schema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(nome,StringType,true), StructField(cognome,StringType,true), StructField(altezza,IntegerType,true), StructField(peso,FloatType,true), StructField(età,IntegerType,true))\r\ndf: org.apache.spark.sql.DataFrame \u003d [nome: string, cognome: string ... 3 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575972831940_75517357",
      "id": "20191210-111351_1574884426",
      "dateCreated": "2019-12-10 11:13:51.940",
      "dateStarted": "2019-12-10 11:50:05.296",
      "dateFinished": "2019-12-10 11:50:05.540",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### How to specify a DDL-formatted schema",
      "user": "admin",
      "dateUpdated": "2019-12-12 19:12:36.064",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eHow to specify a DDL-formatted schema\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575972831076_1219739733",
      "id": "20191210-111351_248682753",
      "dateCreated": "2019-12-10 11:13:51.076",
      "dateStarted": "2019-12-12 19:12:36.064",
      "dateFinished": "2019-12-12 19:12:36.068",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nil metodo **schema** del DataFrameReader consente di specificare lo schema dei dati usando un formato DDL. Esempio:\n`spark.read.schema(\"a INT, b STRING, c DOUBLE\").csv(\"test.csv\")`",
      "user": "anonymous",
      "dateUpdated": "2020-02-16 22:10:42.879",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eil metodo \u003cstrong\u003eschema\u003c/strong\u003e del DataFrameReader consente di specificare lo schema dei dati usando un formato DDL. Esempio:\u003cbr/\u003e\u003ccode\u003espark.read.schema(\u0026quot;a INT, b STRING, c DOUBLE\u0026quot;).csv(\u0026quot;test.csv\u0026quot;)\u003c/code\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575979215236_664173509",
      "id": "20191210-130015_1610535786",
      "dateCreated": "2019-12-10 13:00:15.237",
      "dateStarted": "2020-02-16 22:10:42.880",
      "dateFinished": "2020-02-16 22:10:42.885",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\r\n## DataFrameWriter\r\n\r\n### Write data to the \"core\" data formats (csv, json, jdbc, orc, parquet, text and tables)",
      "user": "admin",
      "dateUpdated": "2019-12-12 19:12:52.961",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eDataFrameWriter\u003c/h2\u003e\n\u003ch3\u003eWrite data to the \u0026ldquo;core\u0026rdquo; data formats (csv, json, jdbc, orc, parquet, text and tables)\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575980609416_-613860425",
      "id": "20191210-132329_1436810178",
      "dateCreated": "2019-12-10 13:23:29.416",
      "dateStarted": "2019-12-12 19:12:52.962",
      "dateFinished": "2019-12-12 19:12:52.967",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nIl modo generico di usare il DataFrameWriter è:\n\n`DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()`\n\nil formato di default è parquet.\n\nQueste sono le varie possibilità:",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 00:26:20.205",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eIl modo generico di usare il DataFrameWriter è:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eDataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eil formato di default è parquet.\u003c/p\u003e\n\u003cp\u003eQueste sono le varie possibilità:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581888107727_623523928",
      "id": "20200216-222147_1203797574",
      "dateCreated": "2020-02-16 22:21:47.727",
      "dateStarted": "2020-02-17 00:26:20.205",
      "dateFinished": "2020-02-17 00:26:20.210",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.write.parquet(folder-path)\r\n\r\ndf.write.orc(folder-path)\r\n\r\ndf.write.json(folder-path)\r\n\r\ndf.write.csv(folder-path)\r\n\r\ndf.write.text(folder-path)",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 14:18:06.564",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1575980608757_-1841819175",
      "id": "20191210-132328_1299705672",
      "dateCreated": "2019-12-10 13:23:28.757",
      "dateStarted": "2019-12-10 13:25:15.250",
      "dateFinished": "2019-12-10 13:25:16.070",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n#### Scrivere un file csv\nEsempio scrittura file tsv (tab separated values):",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 00:27:24.034",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003eScrivere un file csv\u003c/h4\u003e\n\u003cp\u003eEsempio scrittura file tsv (tab separated values):\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581894032383_543544186",
      "id": "20200217-000032_403098763",
      "dateCreated": "2020-02-17 00:00:32.383",
      "dateStarted": "2020-02-17 00:27:24.034",
      "dateFinished": "2020-02-17 00:27:24.039",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\").save(\"/my-tsv-file.tsv\")",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 14:35:49.035",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1575980608445_-966301830",
      "id": "20191210-132328_552244365",
      "dateCreated": "2019-12-10 13:23:28.445",
      "dateStarted": "2019-12-10 14:35:49.084",
      "dateFinished": "2019-12-10 14:35:49.385",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n#### Salvare una tabella\n\nCon **saveAsTable()** il dataframe viene salvato nelle tabella specificata. La tabella sarà pertanto disponibile in *catalog* e verrà salvata nell\u0027hive metastore (di default la cartella spark-warehouse).",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 14:19:19.250",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003eSalvare una tabella\u003c/h4\u003e\n\u003cp\u003eCon \u003cstrong\u003esaveAsTable()\u003c/strong\u003e il dataframe viene salvato nelle tabella specificata. La tabella sarà pertanto disponibile in \u003cem\u003ecatalog\u003c/em\u003e e verrà salvata nell\u0026rsquo;hive metastore (di default la cartella spark-warehouse).\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581894320722_-2096959750",
      "id": "20200217-000520_1814416817",
      "dateCreated": "2020-02-17 00:05:20.722",
      "dateStarted": "2020-02-17 14:19:19.250",
      "dateFinished": "2020-02-17 14:19:19.254",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.write.saveAsTable(\"mytable\")",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 00:05:14.253",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1581894034576_1570799251",
      "id": "20200217-000034_863089926",
      "dateCreated": "2020-02-17 00:00:34.576",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n#### Scrivere un json file\n\nSi noti che il numero di file creati è pari al numero di partizioni del dataframe.",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 14:12:16.871",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003eScrivere un json file\u003c/h4\u003e\n\u003cp\u003eSi noti che il numero di file creati è pari al numero di partizioni del dataframe.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581894756034_19043430",
      "id": "20200217-001236_167095983",
      "dateCreated": "2020-02-17 00:12:36.034",
      "dateStarted": "2020-02-17 14:12:16.871",
      "dateFinished": "2020-02-17 14:12:16.874",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// df.write.json(folder-path)\n// oppure \n\nval df \u003d (1 to 10).toDF()\n\ndf.write.format(\"json\").save(\"d:\\\\AnacondaProjects\\\\testSave\")",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 17:49:43.578",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "df: org.apache.spark.sql.DataFrame \u003d [value: int]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581894815445_-1233619104",
      "id": "20200217-001335_1150908846",
      "dateCreated": "2020-02-17 00:13:35.445",
      "dateStarted": "2020-02-17 17:49:11.379",
      "dateFinished": "2020-02-17 17:49:12.873",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Overwriting existing files",
      "user": "admin",
      "dateUpdated": "2019-12-12 19:13:06.188",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eOverwriting existing files\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575972830189_1970112386",
      "id": "20191210-111350_872219591",
      "dateCreated": "2019-12-10 11:13:50.189",
      "dateStarted": "2019-12-12 19:13:06.189",
      "dateFinished": "2019-12-12 19:13:06.193",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nPossiamo usare il metodo *mode* del DataFrameWriter, per indicare cosa fare in caso di file esistenti, per esempio",
      "user": "anonymous",
      "dateUpdated": "2020-02-16 22:44:13.774",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003ePossiamo usare il metodo \u003cem\u003emode\u003c/em\u003e del DataFrameWriter, per indicare cosa fare in caso di file esistenti, per esempio\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1578916901641_-2032379668",
      "id": "20200113-130141_1585076991",
      "dateCreated": "2020-01-13 13:01:41.641",
      "dateStarted": "2020-02-16 22:44:13.774",
      "dateFinished": "2020-02-16 22:44:13.778",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.write.mode(\"append\").csv(\"data.csv\")",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 13:46:58.168",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1575985003310_810766751",
      "id": "20191210-143643_83693407",
      "dateCreated": "2019-12-10 14:36:43.310",
      "dateStarted": "2020-01-13 13:46:58.233",
      "dateFinished": "2020-01-13 13:46:58.722",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n**.mode()** definisce cosa fare nel caso in cui dati con lo stesso nome son già presenti nella locazione specificata. Le opzioni disponibile sono:\n- *overwrite*: sovrascrive i dati\n- *append*: appende i dati\n- *ignore*: ignora\n- *error*: default, da un errore nel runtime\n",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 13:28:58.059",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003e\u003cstrong\u003e.mode()\u003c/strong\u003e definisce cosa fare nel caso in cui dati con lo stesso nome son già presenti nella locazione specificata. Le opzioni disponibile sono:\u003cbr/\u003e- \u003cem\u003eoverwrite\u003c/em\u003e: sovrascrive i dati\u003cbr/\u003e- \u003cem\u003eappend\u003c/em\u003e: appende i dati\u003cbr/\u003e- \u003cem\u003eignore\u003c/em\u003e: ignora\u003cbr/\u003e- \u003cem\u003eerror\u003c/em\u003e: default, da un errore nel runtime\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1578916973152_893493003",
      "id": "20200113-130253_264468320",
      "dateCreated": "2020-01-13 13:02:53.152",
      "dateStarted": "2020-02-17 13:28:58.059",
      "dateFinished": "2020-02-17 13:28:58.064",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### How to configure options for specific formats",
      "user": "admin",
      "dateUpdated": "2019-12-12 19:13:40.497",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eHow to configure options for specific formats\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575985001854_955862329",
      "id": "20191210-143641_1543266406",
      "dateCreated": "2019-12-10 14:36:41.854",
      "dateStarted": "2019-12-12 19:13:40.497",
      "dateFinished": "2019-12-12 19:13:40.501",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n#### Opzioni per la srittura di un csv file\nLe opzioni per esportare un file in csv sono al [link](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.DataFrameWriter)\n\nQui sotto riportate:\n\n- *sep* (default ,): sets a single character as a separator for each field and value.\n- *quote* (default \"): sets a single character used for escaping quoted values where the separator can be part of the value. If an empty string is set, it uses u0000 (null character).\n- *escape* (default \\): sets a single character used for escaping quotes inside an already quoted value.\n- *charToEscapeQuoteEscaping* (default escape or \\0): sets a single character used for escaping the escape for the quote character. The default value is escape character when escape and quote characters are different, \\0 otherwise.\n- *escapeQuotes* (default true): a flag indicating whether values containing quotes should always be enclosed in quotes. Default is to escape all values containing a quote character.\n- *quoteAll* (default false): a flag indicating whether all values should always be enclosed in quotes. Default is to only escape values containing a quote character.- \n- *header* (default false): writes the names of columns as the first line.\n- *nullValue* (default empty string): sets the string representation of a null value.\n- *emptyValue* (default \"\"): sets the string representation of an empty value.\n- *encoding* (by default it is not set): specifies encoding (charset) of saved csv files. If it is not set, the UTF-8 charset will be used.\n- *compression* (default null): compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, bzip2, gzip, lz4, snappy and deflate).\n- *dateFormat* (default yyyy-MM-dd): sets the string that indicates a date format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to date type.\n- *timestampFormat* (default yyyy-MM-dd\u0027T\u0027HH:mm:ss.SSSXXX): sets the string that indicates a timestamp format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to timestamp type.\n- *ignoreLeadingWhiteSpace* (default true): a flag indicating whether or not leading whitespaces from values being written should be skipped.\n- *ignoreTrailingWhiteSpace* (default true): a flag indicating defines whether or not trailing whitespaces from values being written should be skipped.\n",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 14:19:39.625",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003eOpzioni per la srittura di un csv file\u003c/h4\u003e\n\u003cp\u003eLe opzioni per esportare un file in csv sono al \u003ca href\u003d\"https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.DataFrameWriter\"\u003elink\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eQui sotto riportate:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003cem\u003esep\u003c/em\u003e (default ,): sets a single character as a separator for each field and value.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003equote\u003c/em\u003e (default \u0026quot;): sets a single character used for escaping quoted values where the separator can be part of the value. If an empty string is set, it uses u0000 (null character).\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eescape\u003c/em\u003e (default ): sets a single character used for escaping quotes inside an already quoted value.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003echarToEscapeQuoteEscaping\u003c/em\u003e (default escape or \\0): sets a single character used for escaping the escape for the quote character. The default value is escape character when escape and quote characters are different, \\0 otherwise.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eescapeQuotes\u003c/em\u003e (default true): a flag indicating whether values containing quotes should always be enclosed in quotes. Default is to escape all values containing a quote character.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003equoteAll\u003c/em\u003e (default false): a flag indicating whether all values should always be enclosed in quotes. Default is to only escape values containing a quote character.-\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eheader\u003c/em\u003e (default false): writes the names of columns as the first line.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003enullValue\u003c/em\u003e (default empty string): sets the string representation of a null value.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eemptyValue\u003c/em\u003e (default \u0026quot;\u0026quot;): sets the string representation of an empty value.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eencoding\u003c/em\u003e (by default it is not set): specifies encoding (charset) of saved csv files. If it is not set, the UTF-8 charset will be used.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003ecompression\u003c/em\u003e (default null): compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, bzip2, gzip, lz4, snappy and deflate).\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003edateFormat\u003c/em\u003e (default yyyy-MM-dd): sets the string that indicates a date format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to date type.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003etimestampFormat\u003c/em\u003e (default yyyy-MM-dd\u0026rsquo;T\u0026rsquo;HH:mm:ss.SSSXXX): sets the string that indicates a timestamp format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to timestamp type.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eignoreLeadingWhiteSpace\u003c/em\u003e (default true): a flag indicating whether or not leading whitespaces from values being written should be skipped.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eignoreTrailingWhiteSpace\u003c/em\u003e (default true): a flag indicating defines whether or not trailing whitespaces from values being written should be skipped.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581889995435_-1873111105",
      "id": "20200216-225315_1443504472",
      "dateCreated": "2020-02-16 22:53:15.435",
      "dateStarted": "2020-02-17 14:19:39.626",
      "dateFinished": "2020-02-17 14:19:39.640",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\").save(\"my-tsv-file.tsv\")",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 14:38:47.012",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1575985069916_1826935237",
      "id": "20191210-143749_633513408",
      "dateCreated": "2019-12-10 14:37:49.916",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.write.option(\"sep\", \",\")\n        .option(\"quote\", \"U+005C\")\n        .option(\"escape\", \"U+005C\")\n        .option(\"charToEscapeQuoteEscaping\", \"\\0\")\n        .option(\"escapeQuotes\", \"true\")\n        .option(\"quoteAll\", false)\n        ...\n",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 14:47:54.412",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1575984999899_1715888988",
      "id": "20191210-143639_1616145122",
      "dateCreated": "2019-12-10 14:36:39.900",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n#### Opzioni per la scrittura di un json file\n\nPosso settare i seguenti parametri:\n-- **compression** (default null): definisce l\u0027algoritmo di compressione usato per salvare il file. Si può scegliere tra: none, bzip2, gzip, lz4, snappy and deflate.\n-- **dateFormat** (default yyyy-MM-dd): setta la stringa che definisce il tipo data. I formati per la data sono indicati in java.text.SimpleDateFormat.\n-- **timestampFormat** (default yyyy-MM-dd\u0027T\u0027HH:mm:ss.SSSXXX): setta la stringa che indica il timestampFormat. I formati per il timestamp sono indicati in java.text.SimpleDateFormat.",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 14:19:49.995",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003eOpzioni per la scrittura di un json file\u003c/h4\u003e\n\u003cp\u003ePosso settare i seguenti parametri:\u003cbr/\u003e\u0026ndash; \u003cstrong\u003ecompression\u003c/strong\u003e (default null): definisce l\u0026rsquo;algoritmo di compressione usato per salvare il file. Si può scegliere tra: none, bzip2, gzip, lz4, snappy and deflate.\u003cbr/\u003e\u0026ndash; \u003cstrong\u003edateFormat\u003c/strong\u003e (default yyyy-MM-dd): setta la stringa che definisce il tipo data. I formati per la data sono indicati in java.text.SimpleDateFormat.\u003cbr/\u003e\u0026ndash; \u003cstrong\u003etimestampFormat\u003c/strong\u003e (default yyyy-MM-dd\u0026rsquo;T\u0026rsquo;HH:mm:ss.SSSXXX): setta la stringa che indica il timestampFormat. I formati per il timestamp sono indicati in java.text.SimpleDateFormat.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1578919819424_1250136692",
      "id": "20200113-135019_1114605751",
      "dateCreated": "2020-01-13 13:50:19.424",
      "dateStarted": "2020-02-17 14:19:49.995",
      "dateFinished": "2020-02-17 14:19:50.000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### How to write a data source to 1 single file or N separate files",
      "user": "admin",
      "dateUpdated": "2019-12-12 19:13:53.565",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eHow to write a data source to 1 single file or N separate files\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575985065287_-1685161008",
      "id": "20191210-143745_2125326608",
      "dateCreated": "2019-12-10 14:37:45.287",
      "dateStarted": "2019-12-12 19:13:53.565",
      "dateFinished": "2019-12-12 19:13:53.569",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nDi default Spark salva un numero di files pari al numero di partizioni in cui i dati sono divisi. Se voglio cambiare il numero di files generati devo usare i metodi **.coalesce()** e **.repartition()** per modificare il numero di partizioni del dataframe originario.",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 17:47:06.181",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eDi default Spark salva un numero di files pari al numero di partizioni in cui i dati sono divisi. Se voglio cambiare il numero di files generati devo usare i metodi \u003cstrong\u003e.coalesce()\u003c/strong\u003e e \u003cstrong\u003e.repartition()\u003c/strong\u003e per modificare il numero di partizioni del dataframe originario.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581957001192_-1479614014",
      "id": "20200217-173001_131332749",
      "dateCreated": "2020-02-17 17:30:01.192",
      "dateStarted": "2020-02-17 17:47:06.181",
      "dateFinished": "2020-02-17 17:47:06.189",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.coalesce(1).write.csv(\"D:\\\\Anacondaprojects\\\\prova\")\ndf.repartition(10).write.csv(\"D:\\\\Anacondaprojects\\\\prova2\")",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 17:55:19.825",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1581957150126_1027202108",
      "id": "20200217-173230_48688897",
      "dateCreated": "2020-02-17 17:32:30.126",
      "dateStarted": "2020-02-17 17:54:38.384",
      "dateFinished": "2020-02-17 17:54:39.238",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nPer dividere il file in files più piccoli posso usare il metodo **partitionBy(\"colonna\")**.\nIn questo modo per ogni valore (*key*) della colonna passata come parametro viene generata una cartella dentro la quale viene salvato il file contenente tutti i dati relativi solo alla key corrente, nel formato desiderato, per esempio csv.",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 17:46:12.325",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003ePer dividere il file in files più piccoli posso usare il metodo \u003cstrong\u003epartitionBy(\u0026ldquo;colonna\u0026rdquo;)\u003c/strong\u003e.\u003cbr/\u003eIn questo modo per ogni valore (*key*) della colonna passata come parametro viene generata una cartella dentro la quale viene salvato il file contenente tutti i dati relativi solo alla key corrente, nel formato desiderato, per esempio csv.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1578916013633_766083000",
      "id": "20200113-124653_1841236532",
      "dateCreated": "2020-01-13 12:46:53.633",
      "dateStarted": "2020-02-17 17:46:12.325",
      "dateFinished": "2020-02-17 17:46:12.343",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.write.partitionBy(\"country\").csv(\"./folder1\")",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 12:51:49.047",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1575985839783_380617815",
      "id": "20191210-145039_1021740747",
      "dateCreated": "2019-12-10 14:50:39.783",
      "dateStarted": "2020-01-13 12:47:17.559",
      "dateFinished": "2020-01-13 12:47:19.743",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### How to bucket data by a given set of columns\n",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 15:54:59.313",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eHow to bucket data by a given set of columns\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575985955602_-1704268692",
      "id": "20191210-145235_582686940",
      "dateCreated": "2019-12-10 14:52:35.602",
      "dateStarted": "2020-01-13 15:54:59.314",
      "dateFinished": "2020-01-13 15:54:59.320",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nIl bucketing è una tecnica che usa i buckets (contenitori) per definire il partizionamento dei dati e ridurre lo shuffle dei dati stessi.\n\nSi veda la pagina [Bucketing](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-bucketing.html) su The internal of Spark SQL.\n\nNella pagina linkata è mostrato come facendo il join di due dataframe salvati con il metodo **.bucketBy()** ci si risparmia lo shuffle nell\u0027operazione di join. Si noti che avendo salvato con bucketBy i dati sono pre-shuffled.\n\nPer eseguire il bucketing devo indicare quanti file salvare e su quale colonna di dati fare il partizionamento.",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 18:50:19.040",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eIl bucketing è una tecnica che usa i buckets (contenitori) per definire il partizionamento dei dati e ridurre lo shuffle dei dati stessi.\u003c/p\u003e\n\u003cp\u003eSi veda la pagina \u003ca href\u003d\"https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-bucketing.html\"\u003eBucketing\u003c/a\u003e su The internal of Spark SQL.\u003c/p\u003e\n\u003cp\u003eNella pagina linkata è mostrato come facendo il join di due dataframe salvati con il metodo \u003cstrong\u003e.bucketBy()\u003c/strong\u003e ci si risparmia lo shuffle nell\u0026rsquo;operazione di join. Si noti che avendo salvato con bucketBy i dati sono pre-shuffled.\u003c/p\u003e\n\u003cp\u003ePer eseguire il bucketing devo indicare quanti file salvare e su quale colonna di dati fare il partizionamento.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581960349204_777106737",
      "id": "20200217-182549_1346487714",
      "dateCreated": "2020-02-17 18:25:49.204",
      "dateStarted": "2020-02-17 18:50:19.040",
      "dateFinished": "2020-02-17 18:50:19.047",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df \u003d spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"Datasets/globalpowerplantdatabasev120/*.csv\")\n\ndf.write\n    .mode(\"overwrite\")\n    .bucketBy(14, \"primary_fuel\")\n    .saveAsTable(\"orders_partitioned_bucketed\")",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 18:31:40.209",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "df: org.apache.spark.sql.DataFrame \u003d [country: string, country_long: string ... 22 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581960683927_-92650278",
      "id": "20200217-183123_1548791852",
      "dateCreated": "2020-02-17 18:31:23.927",
      "dateStarted": "2020-02-17 18:31:40.299",
      "dateFinished": "2020-02-17 18:31:41.497",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df \u003d spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"Datasets/globalpowerplantdatabasev120/*.csv\")\r\n\r\ndf.write\r\n    .mode(\"overwrite\")\r\n    .partitionBy(\"country\")\r\n    .bucketBy(14, \"primary_fuel\")\r\n    .saveAsTable(\"orders_partitioned_bucketed\")",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 15:56:05.437",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "df: org.apache.spark.sql.DataFrame \u003d [country: string, country_long: string ... 22 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575986068418_1102276703",
      "id": "20191210-145428_643315789",
      "dateCreated": "2019-12-10 14:54:28.418",
      "dateStarted": "2020-01-13 15:56:05.509",
      "dateFinished": "2020-01-13 15:56:14.781",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "spark.catalog.listTables.show()",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 18:32:16.638",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------------------+--------+-----------+---------+-----------+\n|                name|database|description|tableType|isTemporary|\n+--------------------+--------+-----------+---------+-----------+\n|orders_partitione...| default|       null|  MANAGED|      false|\n+--------------------+--------+-----------+---------+-----------+\n\r\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1578926951267_-344537432",
      "id": "20200113-154911_1783439100",
      "dateCreated": "2020-01-13 15:49:11.267",
      "dateStarted": "2020-02-17 18:32:16.723",
      "dateFinished": "2020-02-17 18:32:16.921",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Differenza tra PartitionBy() e BucketBy()\n\nI metodi *partitioning* e *bucketing* distribuiscono i dati tra diversi file.\n\nCon partitionBy() i dati vengono distribuiti secondo i valori della colonna indicata. Una query fatta su una colonna usata per il partitioning può trarre grosso beneficio dallo stesso. Di contro, se ci sono troppi file, alcune query possono diventare molto lente.\n\nCon bucketBy() invece i dati vengono suddivisi in un numero fissato di file.",
      "user": "anonymous",
      "dateUpdated": "2020-01-13 15:40:53.163",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eDifferenza tra PartitionBy() e BucketBy()\u003c/h3\u003e\n\u003cp\u003eI metodi \u003cem\u003epartitioning\u003c/em\u003e e \u003cem\u003ebucketing\u003c/em\u003e distribuiscono i dati tra diversi file.\u003c/p\u003e\n\u003cp\u003eCon partitionBy() i dati vengono distribuiti secondo i valori della colonna indicata. Una query fatta su una colonna usata per il partitioning può trarre grosso beneficio dallo stesso. Di contro, se ci sono troppi file, alcune query possono diventare molto lente.\u003c/p\u003e\n\u003cp\u003eCon bucketBy() invece i dati vengono suddivisi in un numero fissato di file.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1578924234823_1266872193",
      "id": "20200113-150354_937680943",
      "dateCreated": "2020-01-13 15:03:54.823",
      "dateStarted": "2020-01-13 15:40:53.163",
      "dateFinished": "2020-01-13 15:40:53.204",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## DataFrame\n### Have a working understanding of every action such as take(), collect(), and foreach()",
      "user": "admin",
      "dateUpdated": "2019-12-12 19:14:26.894",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eDataFrame\u003c/h2\u003e\n\u003ch3\u003eHave a working understanding of every action such as take(), collect(), and foreach()\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575986123103_1108148705",
      "id": "20191210-145523_1589489823",
      "dateCreated": "2019-12-10 14:55:23.103",
      "dateStarted": "2019-12-12 19:14:26.894",
      "dateFinished": "2019-12-12 19:14:26.898",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\ncon **.take(n)** ottengo le prime n righe del dataframe",
      "user": "anonymous",
      "dateUpdated": "2020-02-15 21:20:15.162",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003econ \u003cstrong\u003e.take(n)\u003c/strong\u003e ottengo le prime n righe del dataframe\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581797897781_816825793",
      "id": "20200215-211817_578850005",
      "dateCreated": "2020-02-15 21:18:17.781",
      "dateStarted": "2020-02-15 21:20:15.162",
      "dateFinished": "2020-02-15 21:20:15.169",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.take(1)",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 19:04:28.454",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res13: Array[org.apache.spark.sql.Row] \u003d Array([AFG,Afghanistan,Kajaki Hydroelectric Power Plant Afghanistan,GEODB0040538,33.0,32.322,65.119,Hydro,null,null,null,null,null,GEODB,http://globalenergyobservatory.org,GEODB,1009793,2017,null,null,null,null,null,null])\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581797944424_-1595902064",
      "id": "20200215-211904_2142372843",
      "dateCreated": "2020-02-15 21:19:04.424",
      "dateStarted": "2020-02-17 19:04:28.549",
      "dateFinished": "2020-02-17 19:04:28.827",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n**.collect()** invece non prende alcun parametro. come risultato ho tutto il dataframe. Può essere un\u0027operazione \"costosa\" in quanto tutti i dati dagli executor vengono mandati al driver",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 19:05:51.569",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003e\u003cstrong\u003e.collect()\u003c/strong\u003e invece non prende alcun parametro. come risultato ho tutto il dataframe. Può essere un\u0026rsquo;operazione \u0026ldquo;costosa\u0026rdquo; in quanto tutti i dati dagli executor vengono mandati al driver\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581797988337_-103723683",
      "id": "20200215-211948_1014026519",
      "dateCreated": "2020-02-15 21:19:48.337",
      "dateStarted": "2020-02-17 19:05:51.569",
      "dateFinished": "2020-02-17 19:05:51.575",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.collect()",
      "user": "anonymous",
      "dateUpdated": "2020-02-15 21:20:23.034",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res71: Array[org.apache.spark.sql.Row] \u003d Array([1.0,a], [2.0,b], [3.0,c], [1.0,d], [2.0,e], [3.0,f])\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575986121705_1222115163",
      "id": "20191210-145521_1840941900",
      "dateCreated": "2019-12-10 14:55:21.705",
      "dateStarted": "2019-12-10 18:00:46.490",
      "dateFinished": "2019-12-10 18:00:46.759",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Have a working understanding of the various transformations and how they work such as producing a distinct set, filtering data, repartitioning and coalescing, \nperforming joins and unions as well as producing aggregates. ",
      "user": "admin",
      "dateUpdated": "2019-12-12 19:15:21.656",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eHave a working understanding of the various transformations and how they work such as producing a distinct set, filtering data, repartitioning and coalescing,\u003c/h3\u003e\n\u003cp\u003eperforming joins and unions as well as producing aggregates.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575996958299_-1011267694",
      "id": "20191210-175558_1033071390",
      "dateCreated": "2019-12-10 17:55:58.299",
      "dateStarted": "2019-12-12 19:15:21.656",
      "dateFinished": "2019-12-12 19:15:21.661",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n#### FILTERING",
      "user": "admin",
      "dateUpdated": "2019-12-12 19:15:27.229",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003eFILTERING\u003c/h4\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1576106500998_-1406988839",
      "id": "20191212-002140_1784748128",
      "dateCreated": "2019-12-12 00:21:40.998",
      "dateStarted": "2019-12-12 19:15:27.229",
      "dateFinished": "2019-12-12 19:15:27.233",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df \u003d spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"globalpowerplantdatabasev120/*.csv\")",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 18:33:53.232",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "df: org.apache.spark.sql.DataFrame \u003d [country: string, country_long: string ... 22 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575986120376_1104405252",
      "id": "20191210-145520_1389233865",
      "dateCreated": "2019-12-10 14:55:20.376",
      "dateStarted": "2019-12-10 18:33:53.293",
      "dateFinished": "2019-12-10 18:33:54.108",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nAllo scopo di avere una visualizzazione più semplice del dataframe, riduco lo stesso ad un numero di colonne più maneggiabile. Seleziono le colonne contenenti i dati di stato, capacità(MGW), alimentazione, anno di costruzione, produzione di energia stimata (GWh).\nPer farlo uso il metodo **select()**, che agisce sulle colonne.",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 18:14:22.500",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eAllo scopo di avere una visualizzazione più semplice del dataframe, riduco lo stesso ad un numero di colonne più maneggiabile. Seleziono le colonne contenenti i dati di stato, capacità(MGW), alimentazione, anno di costruzione, produzione di energia stimata (GWh).\u003cbr/\u003ePer farlo uso il metodo \u003cstrong\u003eselect()\u003c/strong\u003e, che agisce sulle colonne.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575997753853_-915690360",
      "id": "20191210-180913_1264511170",
      "dateCreated": "2019-12-10 18:09:13.853",
      "dateStarted": "2019-12-10 18:14:22.501",
      "dateFinished": "2019-12-10 18:14:22.509",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df_reduced \u003d df.select(\"country_long\", \"capacity_mw\", \"primary_fuel\", \"commissioning_year\", \"estimated_generation_gwh\")",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 18:50:54.295",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "df_reduced: org.apache.spark.sql.DataFrame \u003d [country_long: string, capacity_mw: double ... 3 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575998033801_-590846949",
      "id": "20191210-181353_632470427",
      "dateCreated": "2019-12-10 18:13:53.801",
      "dateStarted": "2019-12-10 18:50:54.357",
      "dateFinished": "2019-12-10 18:50:54.589",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\ncon **filter()** seleziono solo le righe in cui la produzione stimata è superiore a 50000GWh ",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 18:16:26.724",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003econ \u003cstrong\u003efilter()\u003c/strong\u003e seleziono solo le righe in cui la produzione stimata è superiore a 50000GWh\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575998056843_-2130441077",
      "id": "20191210-181416_307482537",
      "dateCreated": "2019-12-10 18:14:16.843",
      "dateStarted": "2019-12-10 18:16:26.725",
      "dateFinished": "2019-12-10 18:16:26.733",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df_reduced.filter(\u0027estimated_generation_gwh \u003e 50000).show()",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 18:22:23.892",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------------------+-----------+------------+------------------+------------------------+\n|        country_long|capacity_mw|primary_fuel|commissioning_year|estimated_generation_gwh|\n+--------------------+-----------+------------+------------------+------------------------+\n|               China|    13050.0|       Hydro|              null|       53622.49078855527|\n|               China|    22500.0|       Hydro|            2003.0|        92452.5703250953|\n|               China|    12600.0|       Hydro|            2013.0|      51773.439382053366|\n|United States of ...|      454.3|        Coal|              null|       450562.6923495511|\n+--------------------+-----------+------------+------------------+------------------------+\n\r\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575997479838_-170245660",
      "id": "20191210-180439_1208136448",
      "dateCreated": "2019-12-10 18:04:39.838",
      "dateStarted": "2019-12-10 18:22:23.950",
      "dateFinished": "2019-12-10 18:22:24.309",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n#### DISTINCT DATASET",
      "user": "anonymous",
      "dateUpdated": "2019-12-12 00:22:35.706",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003eDISTINCT DATASET\u003c/h4\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1576106533778_-192297514",
      "id": "20191212-002213_234199614",
      "dateCreated": "2019-12-12 00:22:13.778",
      "dateStarted": "2019-12-12 00:22:35.706",
      "dateFinished": "2019-12-12 00:22:35.710",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nuso **distinct()** per vedere quali sono tutti gli elementi distinti della colonna *country_long* ",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 18:25:13.436",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003euso \u003cstrong\u003edistinct()\u003c/strong\u003e per vedere quali sono tutti gli elementi distinti della colonna \u003cem\u003ecountry_long\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575998613849_-1122245355",
      "id": "20191210-182333_246931147",
      "dateCreated": "2019-12-10 18:23:33.849",
      "dateStarted": "2019-12-10 18:25:13.437",
      "dateFinished": "2019-12-10 18:25:13.443",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df_reduced.select(\u0027country_long).distinct().show",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 18:23:20.166",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------------+\n|  country_long|\n+--------------+\n|      Paraguay|\n|        Russia|\n|         Yemen|\n|       Senegal|\n|        Sweden|\n|        Guyana|\n|       Eritrea|\n|   Philippines|\n|      Djibouti|\n|      Malaysia|\n|     Singapore|\n|          Fiji|\n|        Turkey|\n|        Malawi|\n|Western Sahara|\n|          Iraq|\n|       Germany|\n|   Afghanistan|\n|      Cambodia|\n|        Jordan|\n+--------------+\nonly showing top 20 rows\n\r\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575998251455_-2122922514",
      "id": "20191210-181731_80430365",
      "dateCreated": "2019-12-10 18:17:31.455",
      "dateStarted": "2019-12-10 18:23:20.228",
      "dateFinished": "2019-12-10 18:23:22.542",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n#### Fare il JOIN di due dataframe\n\nCiascun dataframe ha il metodo **.join()**, la cui sintassi più semplice è la seguente:\n`dataframe1.join(dataframe2, colonna-comune)`\n\nLa *colonna-comune* è una colonna che ha lo stesso nome in tutti e due i dataframe. \n\nIl default è un inner join, cioè vengono usate le chiavi comuni ai due dataframe nella colonna (comune) specificata.\n\nVediamo un esempio:",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 11:51:00.375",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003eFare il JOIN di due dataframe\u003c/h4\u003e\n\u003cp\u003eCiascun dataframe ha il metodo \u003cstrong\u003e.join()\u003c/strong\u003e, la cui sintassi più semplice è la seguente:\u003cbr/\u003e\u003ccode\u003edataframe1.join(dataframe2, colonna-comune)\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eLa \u003cem\u003ecolonna-comune\u003c/em\u003e è una colonna che ha lo stesso nome in tutti e due i dataframe. \u003c/p\u003e\n\u003cp\u003eIl default è un inner join, cioè vengono usate le chiavi comuni ai due dataframe nella colonna (comune) specificata.\u003c/p\u003e\n\u003cp\u003eVediamo un esempio:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1576106599338_1464784574",
      "id": "20191212-002319_1412952400",
      "dateCreated": "2019-12-12 00:23:19.338",
      "dateStarted": "2020-02-17 11:51:00.376",
      "dateFinished": "2020-02-17 11:51:00.381",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df_1 \u003d Seq(\n    (0, \"zero\"),\n    (1, \"uno\"),\n    (4, \"quattro\")).toDF(\"number\", \"numberString\")\n\nval df_2 \u003d Seq(\n    (1, \"UNO\"),\n    (2, \"due\"),\n    (3, \"tre\")).toDF(\"number\", \"numberString\")\n\n//se ho una chiave comune posso semplicemente indicare la chiave sulla quale fare il join\ndf_2.join(df_1, \"number\").show()",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 23:05:20.321",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------+------------+------------+\n|number|numberString|numberString|\n+------+------------+------------+\n|     1|         UNO|         uno|\n+------+------------+------------+\n\r\ndf_1: org.apache.spark.sql.DataFrame \u003d [number: int, numberString: string]\r\ndf_2: org.apache.spark.sql.DataFrame \u003d [number: int, numberString: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1576014893603_-248154685",
      "id": "20191210-225453_1498492579",
      "dateCreated": "2019-12-10 22:54:53.603",
      "dateStarted": "2019-12-10 23:00:59.277",
      "dateFinished": "2019-12-10 23:01:00.293",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nse non abbiamo una chiave comune devo usare una espressione del tipo \n`$\"key1\"\u003d\u003d\u003d$\"key2\"`",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 11:51:23.884",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003ese non abbiamo una chiave comune devo usare una espressione del tipo\u003cbr/\u003e\u003ccode\u003e$\u0026quot;key1\u0026quot;\u003d\u003d\u003d$\u0026quot;key2\u0026quot;\u003c/code\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1576015631471_966026247",
      "id": "20191210-230711_944681551",
      "dateCreated": "2019-12-10 23:07:11.471",
      "dateStarted": "2020-02-17 11:51:23.885",
      "dateFinished": "2020-02-17 11:51:23.889",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df_1 \u003d Seq(\n    (0, \"zero\"),\n    (1, \"uno\"),\n    (4, \"quattro\")).toDF(\"numberKey1\", \"numberString\")\n\nval df_2 \u003d Seq(\n    (1, \"UNO\"),\n    (2, \"due\"),\n    (3, \"tre\")).toDF(\"numberKey2\", \"numberString\")\n\n//se ho una chiave comune posso semplicemente indicare la chiave sulla quale fare il join\ndf_2.join(df_1, $\"numberKey1\"\u003d\u003d\u003d$\"numberKey2\").show()",
      "user": "anonymous",
      "dateUpdated": "2019-12-10 23:06:50.074",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+----------+------------+----------+------------+\n|numberKey2|numberString|numberKey1|numberString|\n+----------+------------+----------+------------+\n|         1|         UNO|         1|         uno|\n+----------+------------+----------+------------+\n\r\ndf_1: org.apache.spark.sql.DataFrame \u003d [numberKey1: int, numberString: string]\r\ndf_2: org.apache.spark.sql.DataFrame \u003d [numberKey2: int, numberString: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1576015548722_-1750776844",
      "id": "20191210-230548_444257089",
      "dateCreated": "2019-12-10 23:05:48.722",
      "dateStarted": "2019-12-10 23:06:50.129",
      "dateFinished": "2019-12-10 23:06:50.904",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nNon esiste solo l\u0027*inner join* visto sopra, ma possiamo specificare altri tipi di unione. La definizione del metodo **join()** in questo caso è:\n`dataframe1.join(dataframe2, Seq(colonna/e), modo-di-join)`\n\nSi noti che devo:\n\n+ raggruppare le colonne su cui voglio fare il join in una sequenza, anche se si tratta di una sola colonna, \n+ definire la modalità dell\u0027operazione di join, tra:\n    + inner (o innerjoin), \n    + cross, \n    + outer, \n    + full, \n    + full\\_outer (o fullouter), \n    + left o right\n    + left\\_outer o right\\_outer (anche leftouter\\rightouter), \n    + left\\_semi (o leftsemi, \n    + left\\_anti (o leftanti)\n\n\nCon un *join* di tipo **full outer** vengono usate tutte le *key*,comuni e non, dei due dataframe, per le chiavi non comuni si usa **null** dove manca la chiave:",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 13:15:00.655",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eNon esiste solo l\u0026rsquo;\u003cem\u003einner join\u003c/em\u003e visto sopra, ma possiamo specificare altri tipi di unione. La definizione del metodo \u003cstrong\u003ejoin()\u003c/strong\u003e in questo caso è:\u003cbr/\u003e\u003ccode\u003edataframe1.join(dataframe2, Seq(colonna/e), modo-di-join)\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eSi noti che devo:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eraggruppare le colonne su cui voglio fare il join in una sequenza, anche se si tratta di una sola colonna,\u003c/li\u003e\n  \u003cli\u003edefinire la modalità dell\u0026rsquo;operazione di join, tra:\n    \u003cul\u003e\n      \u003cli\u003einner (o innerjoin),\u003c/li\u003e\n      \u003cli\u003ecross,\u003c/li\u003e\n      \u003cli\u003eouter,\u003c/li\u003e\n      \u003cli\u003efull,\u003c/li\u003e\n      \u003cli\u003efull_outer (o fullouter),\u003c/li\u003e\n      \u003cli\u003eleft,\u003c/li\u003e\n      \u003cli\u003eleft_outer,\u003c/li\u003e\n      \u003cli\u003eright,\u003c/li\u003e\n      \u003cli\u003eright_outer,\u003c/li\u003e\n      \u003cli\u003eleft_semi,\u003c/li\u003e\n      \u003cli\u003eleft_anti (o leftanti)\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCon un \u003cem\u003ejoin\u003c/em\u003e di tipo \u003cstrong\u003efull outer\u003c/strong\u003e vengono usate tutte le \u003cem\u003ekey\u003c/em\u003e,comuni e non, dei due dataframe, per le chiavi non comuni si usa \u003cstrong\u003enull\u003c/strong\u003e dove manca la chiave:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581936718387_-600481535",
      "id": "20200217-115158_1087123382",
      "dateCreated": "2020-02-17 11:51:58.387",
      "dateStarted": "2020-02-17 13:11:21.487",
      "dateFinished": "2020-02-17 13:11:21.494",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df_1 \u003d Seq(\n    (0, \"zero\"),\n    (1, \"uno\"),\n    (4, \"quattro\")).toDF(\"number\", \"String1\")\n\nval df_2 \u003d Seq(\n    (1, \"UNO\"),\n    (2, \"due\"),\n    (3, \"tre\")).toDF(\"number\", \"String2\")\n\n//se ho una chiave comune posso semplicemente indicare la chiave sulla quale fare il join\ndf_2.join(df_1, Seq(\"number\"), \"fullouter\").show()",
      "user": "anonymous",
      "dateUpdated": "2020-02-18 11:45:26.894",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------+-------+-------+\n|number|String2|String1|\n+------+-------+-------+\n|     1|    UNO|    uno|\n|     3|    tre|   null|\n|     4|   null|quattro|\n|     2|    due|   null|\n|     0|   null|   zero|\n+------+-------+-------+\n\r\ndf_1: org.apache.spark.sql.DataFrame \u003d [number: int, String1: string]\r\ndf_2: org.apache.spark.sql.DataFrame \u003d [number: int, String2: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1576015524624_-2130056033",
      "id": "20191210-230524_1703954643",
      "dateCreated": "2019-12-10 23:05:24.624",
      "dateStarted": "2020-02-18 11:45:26.987",
      "dateFinished": "2020-02-18 11:45:28.932",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nIn un **leftanti** join vengono usate le chiavi **non comuni** (da qui il nome anti) e vengono prese solo le colonne del dataframe di sinistra",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 12:05:33.957",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eIn un \u003cstrong\u003eleftanti\u003c/strong\u003e join vengono usate le chiavi \u003cstrong\u003enon comuni\u003c/strong\u003e (da qui il nome anti) e vengono prese solo le colonne del dataframe di sinistra\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1576016570059_-1544750330",
      "id": "20191210-232250_794208794",
      "dateCreated": "2019-12-10 23:22:50.059",
      "dateStarted": "2020-02-17 12:05:33.958",
      "dateFinished": "2020-02-17 12:05:33.962",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df_2.join(df_1, Seq(\"number\"), \"leftanti\").show()",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 13:07:47.913",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------+------------+\n|number|numberString|\n+------+------------+\n|     2|         due|\n|     3|         tre|\n+------+------------+\n\r\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581937427157_-724959798",
      "id": "20200217-120347_197948903",
      "dateCreated": "2020-02-17 12:03:47.157",
      "dateStarted": "2020-02-17 13:07:47.999",
      "dateFinished": "2020-02-17 13:07:48.248",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nCon **leftouter** vengono usate tutte le *key*, comuni e non, del dataframe di sinistra:",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 13:11:33.404",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eCon \u003cstrong\u003eleftouter\u003c/strong\u003e vengono usate tutte le \u003cem\u003ekey\u003c/em\u003e, comuni e non, del dataframe di sinistra:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581941382244_-1045769394",
      "id": "20200217-130942_158523657",
      "dateCreated": "2020-02-17 13:09:42.244",
      "dateStarted": "2020-02-17 13:11:33.405",
      "dateFinished": "2020-02-17 13:11:33.408",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df_2.join(df_1, Seq(\"number\"), \"leftouter\").show()",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 13:14:16.902",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------+------------+------------+\n|number|numberString|numberString|\n+------+------------+------------+\n|     0|        null|        zero|\n|     1|         UNO|         uno|\n|     4|        null|     quattro|\n+------+------------+------------+\n\r\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581937627849_-855248617",
      "id": "20200217-120707_1008485631",
      "dateCreated": "2020-02-17 12:07:07.850",
      "dateStarted": "2020-02-17 13:14:08.425",
      "dateFinished": "2020-02-17 13:14:08.668",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\ncon **leftsemi** si prendono le chiavi comuni ai due dataframe, il dataframe risultate avrà solo la colonna del dataframe left:",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 13:13:04.634",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003econ \u003cstrong\u003eleftsemi\u003c/strong\u003e si prendono le chiavi comuni ai due dataframe, il dataframe risultate avrà solo la colonna del dataframe left:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581941515016_-877563751",
      "id": "20200217-131155_525395835",
      "dateCreated": "2020-02-17 13:11:55.016",
      "dateStarted": "2020-02-17 13:13:04.635",
      "dateFinished": "2020-02-17 13:13:04.638",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df_2.join(df_1, Seq(\"number\"), \"leftsemi\").show()",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 13:13:11.672",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------+------------+\n|number|numberString|\n+------+------------+\n|     1|         UNO|\n+------+------------+\n\r\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1576016859529_1855191508",
      "id": "20191210-232739_1603140433",
      "dateCreated": "2019-12-10 23:27:39.530",
      "dateStarted": "2020-02-17 13:13:11.761",
      "dateFinished": "2020-02-17 13:13:12.005",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n#### UNION\n\nL\u0027unione è una sorta di join in verticale di due dataframe.\nSi noti che, tutto quello che il comando fa è di unire in verticale i due dataframe senza ocuparsi della \"coerenza\" tra i due dataframe, nell\u0027esempio successivo i le colonne dei due dataframe hanno nomi (e probabilmente significati) diversi, ciononostante l\u0027unione viene fatta senza alcun errore.",
      "user": "anonymous",
      "dateUpdated": "2020-02-18 12:03:49.373",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003eUNION\u003c/h4\u003e\n\u003cp\u003eL\u0026rsquo;unione è una sorta di join in verticale di due dataframe.\u003cbr/\u003eSi noti che, tutto quello che il comando fa è di unire in verticale i due dataframe senza ocuparsi della \u0026ldquo;coerenza\u0026rdquo; tra i due dataframe, nell\u0026rsquo;esempio successivo i le colonne dei due dataframe hanno nomi (e probabilmente significati) diversi, ciononostante l\u0026rsquo;unione viene fatta senza alcun errore.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1582021883778_-7376005",
      "id": "20200218-113123_2052298145",
      "dateCreated": "2020-02-18 11:31:23.778",
      "dateStarted": "2020-02-18 12:03:49.373",
      "dateFinished": "2020-02-18 12:03:49.390",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df_2.union(df_1).show()",
      "user": "anonymous",
      "dateUpdated": "2020-02-18 11:51:54.353",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------+-------+\n|number|String2|\n+------+-------+\n|     1|    UNO|\n|     2|    due|\n|     3|    tre|\n|     0|   zero|\n|     1|    uno|\n|     4|quattro|\n+------+-------+\n\r\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1582021906107_-2039223540",
      "id": "20200218-113146_1451745562",
      "dateCreated": "2020-02-18 11:31:46.107",
      "dateStarted": "2020-02-18 11:50:10.394",
      "dateFinished": "2020-02-18 11:50:10.665",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n#### AGGREGATES\nCon le funzioni di aggregazioni, come indica la denominazione stessa, posso fare una aggregazione **della colonna** (intera) in un valore, per esempio un valore massimo o una media etc. Le funzioni disponibile per l\u0027aggregazione sono al link [RelationalGroupedDataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)",
      "user": "anonymous",
      "dateUpdated": "2020-02-18 12:10:03.133",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003eAGGREGATES\u003c/h4\u003e\n\u003cp\u003eCon le funzioni di aggregazioni, come indica la denominazione stessa, posso fare una aggregazione \u003cstrong\u003edella colonna\u003c/strong\u003e (intera) in un valore, per esempio un valore massimo o una media etc. Le funzioni disponibile per l\u0026rsquo;aggregazione sono al link \u003ca href\u003d\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset\"\u003eRelationalGroupedDataset\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1582023832883_-2146078412",
      "id": "20200218-120352_191075215",
      "dateCreated": "2020-02-18 12:03:52.883",
      "dateStarted": "2020-02-18 12:10:03.134",
      "dateFinished": "2020-02-18 12:10:03.145",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df_2.agg(max($\"number\")).show",
      "user": "anonymous",
      "dateUpdated": "2020-02-18 12:11:37.056",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------+\n|max(number)|\n+-----------+\n|          3|\n+-----------+\n\r\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1582024209168_-1000864329",
      "id": "20200218-121009_1515156648",
      "dateCreated": "2020-02-18 12:10:09.168",
      "dateStarted": "2020-02-18 12:11:37.144",
      "dateFinished": "2020-02-18 12:11:37.456",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n#### PARTINIONING - REPARTITION e COALESCE",
      "user": "anonymous",
      "dateUpdated": "2019-12-12 00:24:50.707",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003ePARTINIONING - REPARTITION e COALESCE\u003c/h4\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1576106573718_-1583946594",
      "id": "20191212-002253_416496746",
      "dateCreated": "2019-12-12 00:22:53.718",
      "dateStarted": "2019-12-12 00:24:50.707",
      "dateFinished": "2019-12-12 00:24:50.711",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nCon **.coalesce()** si può ridurre il numero di partizioni minimizzando lo shuffling dei dati.\n\nCon **.repartition()** posso aumentare o ridurre il numero delle partizione, e ho un reshuffling completo dei dati.\n\nVediamo come usare questi due comandi.\n\nInnanzitutto allo scopo di dimostrare l\u0027uso di coalesce voglio creare un numero alto di partizioni pertanto setto la dimensione massima della partizione a 1MB",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 13:15:01.236",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eCon \u003cstrong\u003e.coalesce()\u003c/strong\u003e si può ridurre il numero di partizioni minimizzando lo shuffling dei dati.\u003c/p\u003e\n\u003cp\u003eCon \u003cstrong\u003e.repartition()\u003c/strong\u003e posso aumentare o ridurre il numero delle partizione, e ho un reshuffling completo dei dati.\u003c/p\u003e\n\u003cp\u003eVediamo come usare questi due comandi.\u003c/p\u003e\n\u003cp\u003eInnanzitutto allo scopo di dimostrare l\u0026rsquo;uso di coalesce voglio creare un numero alto di partizioni pertanto setto la dimensione massima della partizione a 1MB\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581934407516_-1976477177",
      "id": "20200217-111327_279047721",
      "dateCreated": "2020-02-17 11:13:27.516",
      "dateStarted": "2020-02-17 13:15:01.236",
      "dateFinished": "2020-02-17 13:15:01.241",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", 1*1024*1024)",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 16:33:26.317",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1581935173233_-86326167",
      "id": "20200217-112613_220032461",
      "dateCreated": "2020-02-17 11:26:13.233",
      "dateStarted": "2020-02-17 16:33:26.400",
      "dateFinished": "2020-02-17 16:33:26.602",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nCostruisco un dataframe e controllo su quante partizioni è distribuito:",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 17:45:23.938",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eCostruisco un dataframe e controllo su quante partizioni è distribuito:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581935188786_-903934363",
      "id": "20200217-112628_1655342516",
      "dateCreated": "2020-02-17 11:26:28.786",
      "dateStarted": "2020-02-17 11:26:55.368",
      "dateFinished": "2020-02-17 11:26:55.374",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df \u003d spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"D:\\\\AnacondaProjects\\\\Datasets\\\\globalpowerplantdatabasev120\\\\*.csv\")\n\nprintln(\"Numero di partizioni di df: \" + df.rdd.getNumPartitions)",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 16:33:53.142",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Numero di partizioni di df: 8\r\ndf: org.apache.spark.sql.DataFrame \u003d [country: string, country_long: string ... 22 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581935219968_-1969635621",
      "id": "20200217-112659_1679587844",
      "dateCreated": "2020-02-17 11:26:59.968",
      "dateStarted": "2020-02-17 16:33:53.227",
      "dateFinished": "2020-02-17 16:33:53.787",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nriduco il numero di partizioni a 4, usando coalesce mi garantisco che sia fatto il minur numero di sposatmento dei dati",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 11:34:56.911",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eriduco il numero di partizioni a 4, usando coalesce mi garantisco che sia fatto il minur numero di sposatmento dei dati\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581935684823_-828117999",
      "id": "20200217-113444_1717515116",
      "dateCreated": "2020-02-17 11:34:44.823",
      "dateStarted": "2020-02-17 11:34:56.913",
      "dateFinished": "2020-02-17 11:34:56.918",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df_coalesce \u003d df.coalesce(4)\n\nprintln(\"Numero di partizioni di df_coalesce: \" + df_coalesce.rdd.getNumPartitions)",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 11:35:22.350",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Numero di partizioni di df_coalesce: 4\r\ndf_coalesce: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [country: string, country_long: string ... 22 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581935705357_766742596",
      "id": "20200217-113505_966786034",
      "dateCreated": "2020-02-17 11:35:05.357",
      "dateStarted": "2020-02-17 11:35:22.351",
      "dateFinished": "2020-02-17 11:35:22.626",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\naumento il numero di partizioni a 12",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 11:35:44.571",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eaumento il numero di partizioni a 12\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581935737668_1651328902",
      "id": "20200217-113537_2129940739",
      "dateCreated": "2020-02-17 11:35:37.668",
      "dateStarted": "2020-02-17 11:35:44.572",
      "dateFinished": "2020-02-17 11:35:44.575",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df_repartition \u003d df.repartition(12)\n\nprintln(\"Numero di partizioni di df_repartition: \" + df_repartition.rdd.getNumPartitions)",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 11:42:51.801",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 226.8,
              "optionOpen": false
            }
          }
        },
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Numero di partizioni di df_repartition: 12\r\ndf_repartition: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [country: string, country_long: string ... 22 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575998769596_-1291444881",
      "id": "20191210-182609_462097691",
      "dateCreated": "2019-12-10 18:26:09.596",
      "dateStarted": "2020-02-17 11:42:51.889",
      "dateFinished": "2020-02-17 11:42:52.115",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Altro",
      "user": "anonymous",
      "dateUpdated": "2020-02-15 23:07:36.047",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eAltro\u003c/h2\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581804434093_-258269718",
      "id": "20200215-230714_1278151703",
      "dateCreated": "2020-02-15 23:07:14.093",
      "dateStarted": "2020-02-15 23:07:36.048",
      "dateFinished": "2020-02-15 23:07:36.055",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Salvare e leggere una tabella Hive con la SparkSession\n\nEsempio preso dal sito Databricks a questo [link](https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html)\n\n```\n//drop the table if exists to get around existing table error\nspark.sql(\"DROP TABLE IF EXISTS zips_hive_table\")\n//save as a hive table\nspark.table(\"zips_table\").write.saveAsTable(\"zips_hive_table\")\n//make a similar query against the hive table \nval resultsHiveDF \u003d spark.sql(\"SELECT city, pop, state, zip FROM zips_hive_table WHERE pop \u003e 40000\")\nresultsHiveDF.show(10)\n```",
      "user": "anonymous",
      "dateUpdated": "2020-02-17 13:15:20.677",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eSalvare e leggere una tabella Hive con la SparkSession\u003c/h3\u003e\n\u003cp\u003eEsempio preso dal sito Databricks a questo \u003ca href\u003d\"https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html\"\u003elink\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e//drop the table if exists to get around existing table error\nspark.sql(\u0026quot;DROP TABLE IF EXISTS zips_hive_table\u0026quot;)\n//save as a hive table\nspark.table(\u0026quot;zips_table\u0026quot;).write.saveAsTable(\u0026quot;zips_hive_table\u0026quot;)\n//make a similar query against the hive table \nval resultsHiveDF \u003d spark.sql(\u0026quot;SELECT city, pop, state, zip FROM zips_hive_table WHERE pop \u0026gt; 40000\u0026quot;)\nresultsHiveDF.show(10)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1581804605181_-2117837000",
      "id": "20200215-231005_871768963",
      "dateCreated": "2020-02-15 23:10:05.181",
      "dateStarted": "2020-02-17 13:15:20.677",
      "dateFinished": "2020-02-17 13:15:20.681",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "user": "anonymous",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1581804611872_-1781922271",
      "id": "20200215-231011_439470249",
      "dateCreated": "2020-02-15 23:10:11.872",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Apache Spark - Databricks certification",
  "id": "2EUA7CSBC",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}